{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e55867f",
   "metadata": {},
   "source": [
    "# Лабораторная работа № 1\n",
    "### Боровских Вадим, 932003\n",
    "## С) Регрессор  DS_2019_public.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af251541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73024bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Climate_Region_Pub</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>REPORTABLE_DOMAIN</th>\n",
       "      <th>DOLELCOL</th>\n",
       "      <th>TOTALDOLCOL</th>\n",
       "      <th>KWHCOL</th>\n",
       "      <th>BTUELCOL</th>\n",
       "      <th>TOTALBTUCOL</th>\n",
       "      <th>TOTALDOLSPH</th>\n",
       "      <th>TOTALBTUSPH</th>\n",
       "      <th>...</th>\n",
       "      <th>LGT1EE</th>\n",
       "      <th>TOTALBTUWTH</th>\n",
       "      <th>ROOFTYPE</th>\n",
       "      <th>DOLELRFG</th>\n",
       "      <th>TOTALDOLRFG</th>\n",
       "      <th>HEATROOM</th>\n",
       "      <th>WDWATER</th>\n",
       "      <th>UGWARM</th>\n",
       "      <th>DRYRFUEL</th>\n",
       "      <th>KWHRFG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>16.793</td>\n",
       "      <td>17</td>\n",
       "      <td>181.998</td>\n",
       "      <td>620.979</td>\n",
       "      <td>621</td>\n",
       "      <td>368</td>\n",
       "      <td>38606</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>30110</td>\n",
       "      <td>2</td>\n",
       "      <td>120.893</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1310.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48.901</td>\n",
       "      <td>49</td>\n",
       "      <td>184.459</td>\n",
       "      <td>629.389</td>\n",
       "      <td>629</td>\n",
       "      <td>582</td>\n",
       "      <td>40248</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7933</td>\n",
       "      <td>-2</td>\n",
       "      <td>242.746</td>\n",
       "      <td>243</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>915.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>101.048</td>\n",
       "      <td>101</td>\n",
       "      <td>1063.022</td>\n",
       "      <td>3627.013</td>\n",
       "      <td>3627</td>\n",
       "      <td>425</td>\n",
       "      <td>40196</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>6440</td>\n",
       "      <td>5</td>\n",
       "      <td>158.797</td>\n",
       "      <td>159</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1670.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>36136</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>13884</td>\n",
       "      <td>5</td>\n",
       "      <td>85.138</td>\n",
       "      <td>85</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>346.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>45.132</td>\n",
       "      <td>45</td>\n",
       "      <td>274.530</td>\n",
       "      <td>936.677</td>\n",
       "      <td>937</td>\n",
       "      <td>685</td>\n",
       "      <td>74100</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>14204</td>\n",
       "      <td>6</td>\n",
       "      <td>151.319</td>\n",
       "      <td>151</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>920.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>345.8</td>\n",
       "      <td>346</td>\n",
       "      <td>2695.625</td>\n",
       "      <td>9197.516</td>\n",
       "      <td>9198</td>\n",
       "      <td>820</td>\n",
       "      <td>26863</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>7179</td>\n",
       "      <td>5</td>\n",
       "      <td>264.409</td>\n",
       "      <td>264</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2061.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>13.005</td>\n",
       "      <td>13</td>\n",
       "      <td>97.497</td>\n",
       "      <td>332.665</td>\n",
       "      <td>333</td>\n",
       "      <td>521</td>\n",
       "      <td>49610</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>6471</td>\n",
       "      <td>6</td>\n",
       "      <td>75.569</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>566.544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>97.67</td>\n",
       "      <td>98</td>\n",
       "      <td>847.734</td>\n",
       "      <td>2892.504</td>\n",
       "      <td>2893</td>\n",
       "      <td>591</td>\n",
       "      <td>70626</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>24391</td>\n",
       "      <td>6</td>\n",
       "      <td>250.313</td>\n",
       "      <td>250</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2172.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>12.834</td>\n",
       "      <td>13</td>\n",
       "      <td>135.687</td>\n",
       "      <td>462.975</td>\n",
       "      <td>463</td>\n",
       "      <td>432</td>\n",
       "      <td>49821</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>12416</td>\n",
       "      <td>6</td>\n",
       "      <td>175.135</td>\n",
       "      <td>175</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1851.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>21764</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>8017</td>\n",
       "      <td>6</td>\n",
       "      <td>49.364</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>418.766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10875 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Climate_Region_Pub  DIVISION  REPORTABLE_DOMAIN DOLELCOL  TOTALDOLCOL  \\\n",
       "0                       5        10                 26   16.793           17   \n",
       "1                       1         1                  1   48.901           49   \n",
       "2                       1         3                  7  101.048          101   \n",
       "3                       1         1                  1        0            0   \n",
       "4                       1         4                 10   45.132           45   \n",
       "...                   ...       ...                ...      ...          ...   \n",
       "10870                   4         5                 13    345.8          346   \n",
       "10871                   1         3                  9   13.005           13   \n",
       "10872                   1         4                 10    97.67           98   \n",
       "10873                   1         8                 23   12.834           13   \n",
       "10874                   5        10                 26        0            0   \n",
       "\n",
       "         KWHCOL  BTUELCOL  TOTALBTUCOL  TOTALDOLSPH  TOTALBTUSPH  ...  LGT1EE  \\\n",
       "0       181.998   620.979          621          368        38606  ...      -2   \n",
       "1       184.459   629.389          629          582        40248  ...       2   \n",
       "2      1063.022  3627.013         3627          425        40196  ...      -2   \n",
       "3         0.000     0.000            0          616        36136  ...       2   \n",
       "4       274.530   936.677          937          685        74100  ...       3   \n",
       "...         ...       ...          ...          ...          ...  ...     ...   \n",
       "10870  2695.625  9197.516         9198          820        26863  ...       8   \n",
       "10871    97.497   332.665          333          521        49610  ...       4   \n",
       "10872   847.734  2892.504         2893          591        70626  ...      -2   \n",
       "10873   135.687   462.975          463          432        49821  ...       0   \n",
       "10874     0.000     0.000            0          236        21764  ...       3   \n",
       "\n",
       "       TOTALBTUWTH  ROOFTYPE  DOLELRFG  TOTALDOLRFG  HEATROOM  WDWATER  \\\n",
       "0            30110         2   120.893          121         4        0   \n",
       "1             7933        -2   242.746          243         2        0   \n",
       "2             6440         5   158.797          159         7        0   \n",
       "3            13884         5    85.138           85         5        0   \n",
       "4            14204         6   151.319          151         6        0   \n",
       "...            ...       ...       ...          ...       ...      ...   \n",
       "10870         7179         5   264.409          264         7        0   \n",
       "10871         6471         6    75.569           76         5        0   \n",
       "10872        24391         6   250.313          250         8        0   \n",
       "10873        12416         6   175.135          175         5        0   \n",
       "10874         8017         6    49.364           49         4        0   \n",
       "\n",
       "       UGWARM  DRYRFUEL    KWHRFG  \n",
       "0           1         1  1310.220  \n",
       "1           1        -2   915.664  \n",
       "2           1         5  1670.534  \n",
       "3           1         5   346.468  \n",
       "4           1         5   920.454  \n",
       "...       ...       ...       ...  \n",
       "10870       0         5  2061.159  \n",
       "10871       1         5   566.544  \n",
       "10872       1         1  2172.621  \n",
       "10873       1         1  1851.648  \n",
       "10874       1         5   418.766  \n",
       "\n",
       "[10875 rows x 121 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"DS_2019_public.csv\", index_col=0, encoding='cp1252', dtype={col: 'str' for col in [30,56,96,114]})\n",
    "df=df.reset_index() \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d35cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Climate_Region_Pub', 'DIVISION', 'REPORTABLE_DOMAIN', 'DOLELCOL',\n",
       "       'TOTALDOLCOL', 'KWHCOL', 'BTUELCOL', 'TOTALBTUCOL', 'TOTALDOLSPH',\n",
       "       'TOTALBTUSPH',\n",
       "       ...\n",
       "       'LGT1EE', 'TOTALBTUWTH', 'ROOFTYPE', 'DOLELRFG', 'TOTALDOLRFG',\n",
       "       'HEATROOM', 'WDWATER', 'UGWARM', 'DRYRFUEL', 'KWHRFG'],\n",
       "      dtype='object', length=121)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a7d41b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Climate_Region_Pub      int64\n",
       "DIVISION                int64\n",
       "REPORTABLE_DOMAIN       int64\n",
       "DOLELCOL               object\n",
       "TOTALDOLCOL             int64\n",
       "                       ...   \n",
       "HEATROOM                int64\n",
       "WDWATER                 int64\n",
       "UGWARM                  int64\n",
       "DRYRFUEL                int64\n",
       "KWHRFG                float64\n",
       "Length: 121, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6140731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10875 entries, 0 to 10874\n",
      "Columns: 121 entries, Climate_Region_Pub to KWHRFG\n",
      "dtypes: float64(34), int64(79), object(8)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d173b33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Climate_Region_Pub</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>REPORTABLE_DOMAIN</th>\n",
       "      <th>TOTALDOLCOL</th>\n",
       "      <th>KWHCOL</th>\n",
       "      <th>BTUELCOL</th>\n",
       "      <th>TOTALBTUCOL</th>\n",
       "      <th>TOTALDOLSPH</th>\n",
       "      <th>TOTALBTUSPH</th>\n",
       "      <th>CELLAR</th>\n",
       "      <th>...</th>\n",
       "      <th>TEMPGONE</th>\n",
       "      <th>LGT1EE</th>\n",
       "      <th>TOTALBTUWTH</th>\n",
       "      <th>ROOFTYPE</th>\n",
       "      <th>TOTALDOLRFG</th>\n",
       "      <th>HEATROOM</th>\n",
       "      <th>WDWATER</th>\n",
       "      <th>UGWARM</th>\n",
       "      <th>DRYRFUEL</th>\n",
       "      <th>KWHRFG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "      <td>10875.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.601195</td>\n",
       "      <td>5.371034</td>\n",
       "      <td>14.778391</td>\n",
       "      <td>202.429333</td>\n",
       "      <td>1682.782696</td>\n",
       "      <td>5741.654205</td>\n",
       "      <td>5741.650943</td>\n",
       "      <td>561.501517</td>\n",
       "      <td>36931.489103</td>\n",
       "      <td>-0.082943</td>\n",
       "      <td>...</td>\n",
       "      <td>63.789609</td>\n",
       "      <td>0.818115</td>\n",
       "      <td>16313.564138</td>\n",
       "      <td>3.675126</td>\n",
       "      <td>154.358345</td>\n",
       "      <td>5.347126</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.501793</td>\n",
       "      <td>2.984184</td>\n",
       "      <td>1244.033159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.349507</td>\n",
       "      <td>2.862200</td>\n",
       "      <td>8.207299</td>\n",
       "      <td>310.691148</td>\n",
       "      <td>2480.831034</td>\n",
       "      <td>8464.594227</td>\n",
       "      <td>8464.591405</td>\n",
       "      <td>505.921369</td>\n",
       "      <td>35823.762607</td>\n",
       "      <td>1.066451</td>\n",
       "      <td>...</td>\n",
       "      <td>14.406053</td>\n",
       "      <td>2.346643</td>\n",
       "      <td>13733.416871</td>\n",
       "      <td>2.775507</td>\n",
       "      <td>106.994526</td>\n",
       "      <td>2.636779</td>\n",
       "      <td>0.028757</td>\n",
       "      <td>0.500020</td>\n",
       "      <td>2.841844</td>\n",
       "      <td>778.588285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>143.161500</td>\n",
       "      <td>488.477000</td>\n",
       "      <td>488.500000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>8748.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>723.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>748.220000</td>\n",
       "      <td>2552.933000</td>\n",
       "      <td>2553.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>27289.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12761.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1047.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>2281.322500</td>\n",
       "      <td>7783.797500</td>\n",
       "      <td>7784.000000</td>\n",
       "      <td>762.500000</td>\n",
       "      <td>55784.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21169.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1558.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>7729.000000</td>\n",
       "      <td>60995.431000</td>\n",
       "      <td>208116.552000</td>\n",
       "      <td>208117.000000</td>\n",
       "      <td>9264.000000</td>\n",
       "      <td>548711.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>284130.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2490.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11069.027000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Climate_Region_Pub      DIVISION  REPORTABLE_DOMAIN   TOTALDOLCOL  \\\n",
       "count        10875.000000  10875.000000       10875.000000  10875.000000   \n",
       "mean             2.601195      5.371034          14.778391    202.429333   \n",
       "std              1.349507      2.862200           8.207299    310.691148   \n",
       "min              1.000000      1.000000           1.000000      0.000000   \n",
       "25%              1.000000      3.000000           8.000000     18.000000   \n",
       "50%              3.000000      5.000000          15.000000     90.000000   \n",
       "75%              4.000000      7.000000          21.000000    263.000000   \n",
       "max              5.000000     10.000000          27.000000   7729.000000   \n",
       "\n",
       "             KWHCOL       BTUELCOL    TOTALBTUCOL   TOTALDOLSPH  \\\n",
       "count  10875.000000   10875.000000   10875.000000  10875.000000   \n",
       "mean    1682.782696    5741.654205    5741.650943    561.501517   \n",
       "std     2480.831034    8464.594227    8464.591405    505.921369   \n",
       "min        0.000000       0.000000       0.000000      0.000000   \n",
       "25%      143.161500     488.477000     488.500000    222.000000   \n",
       "50%      748.220000    2552.933000    2553.000000    431.000000   \n",
       "75%     2281.322500    7783.797500    7784.000000    762.500000   \n",
       "max    60995.431000  208116.552000  208117.000000   9264.000000   \n",
       "\n",
       "         TOTALBTUSPH        CELLAR  ...      TEMPGONE        LGT1EE  \\\n",
       "count   10875.000000  10875.000000  ...  10875.000000  10875.000000   \n",
       "mean    36931.489103     -0.082943  ...     63.789609      0.818115   \n",
       "std     35823.762607      1.066451  ...     14.406053      2.346643   \n",
       "min         0.000000     -2.000000  ...     -2.000000     -9.000000   \n",
       "25%      8748.500000      0.000000  ...     62.000000      0.000000   \n",
       "50%     27289.000000      0.000000  ...     67.000000      0.000000   \n",
       "75%     55784.500000      1.000000  ...     70.000000      2.000000   \n",
       "max    548711.000000      1.000000  ...     90.000000     40.000000   \n",
       "\n",
       "         TOTALBTUWTH      ROOFTYPE   TOTALDOLRFG      HEATROOM       WDWATER  \\\n",
       "count   10875.000000  10875.000000  10875.000000  10875.000000  10875.000000   \n",
       "mean    16313.564138      3.675126    154.358345      5.347126      0.000828   \n",
       "std     13733.416871      2.775507    106.994526      2.636779      0.028757   \n",
       "min         0.000000     -2.000000      0.000000     -2.000000      0.000000   \n",
       "25%      7708.000000      2.000000     86.000000      4.000000      0.000000   \n",
       "50%     12761.000000      5.000000    127.000000      5.000000      0.000000   \n",
       "75%     21169.500000      5.000000    193.000000      7.000000      0.000000   \n",
       "max    284130.000000      8.000000   2490.000000     23.000000      1.000000   \n",
       "\n",
       "             UGWARM      DRYRFUEL        KWHRFG  \n",
       "count  10875.000000  10875.000000  10875.000000  \n",
       "mean       0.501793      2.984184   1244.033159  \n",
       "std        0.500020      2.841844    778.588285  \n",
       "min        0.000000     -2.000000      0.000000  \n",
       "25%        0.000000      1.000000    723.870500  \n",
       "50%        1.000000      5.000000   1047.355000  \n",
       "75%        1.000000      5.000000   1558.283000  \n",
       "max        1.000000      5.000000  11069.027000  \n",
       "\n",
       "[8 rows x 113 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e224e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Удалите строки с некорректными значениями\n",
    "df = df[~df.apply(lambda row: row.astype(str).str.contains('[^0-9.]').any(), axis=1)]\n",
    "# Затем выполните масштабирование\n",
    "X = df.drop(['TOTALBTUCOL'], axis=1)\n",
    "y = df['TOTALBTUCOL']\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5a288",
   "metadata": {},
   "source": [
    "### Разделение данных на обучающую и тестовую выборки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff712662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : (3976, 120), Test : (995, 120)\n",
      "Train : (3180, 120), Test : (796, 120)\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=0)\n",
    "print(f'Train : {X_train_val.shape}, Test : {X_test.shape}')\n",
    "print(f'Train : {X_train.shape}, Test : {X_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0e27c",
   "metadata": {},
   "source": [
    "### Построение модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "744f0db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                7260      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21841 (85.32 KB)\n",
      "Trainable params: 21841 (85.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "regressor = Sequential()\n",
    "regressor.add(Dense(120, activation='relu', input_dim=X_train.shape[1]))\n",
    "regressor.add(Dense(60,activation='relu'))\n",
    "regressor.add(Dense(1,activation='linear'))\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b20ee300",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(loss='mse', optimizer='adam', metrics='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "421a0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)\n",
    "checkpoint = ModelCheckpoint('regressor_weights-{epoch:02d}-{val_loss:.3f}.hdf5',\n",
    "monitor='val_loss', verbose=1, mode='min', save_best_only=True)\n",
    "callbacks_list = [early_stop, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28223e",
   "metadata": {},
   "source": [
    "### Обучение модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "322d9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 73595.1016 - mae: 177.5685\n",
      "Epoch 1: val_loss improved from 91159.12500 to 88805.09375, saving model to regressor_weights-01-88805.094.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 79100.7031 - mae: 178.3285 - val_loss: 88805.0938 - val_mae: 185.0776\n",
      "Epoch 2/350\n",
      "36/80 [============>.................] - ETA: 0s - loss: 83602.2031 - mae: 185.7291"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/80 [==========================>...] - ETA: 0s - loss: 81987.7500 - mae: 179.8768\n",
      "Epoch 2: val_loss improved from 88805.09375 to 85150.42969, saving model to regressor_weights-02-85150.430.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 78983.8438 - mae: 177.4560 - val_loss: 85150.4297 - val_mae: 181.1830\n",
      "Epoch 3/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 80904.6797 - mae: 179.5310\n",
      "Epoch 3: val_loss did not improve from 85150.42969\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 79744.4062 - mae: 178.4410 - val_loss: 87148.1172 - val_mae: 180.1727\n",
      "Epoch 4/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 79679.9922 - mae: 180.1443\n",
      "Epoch 4: val_loss did not improve from 85150.42969\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 79679.9922 - mae: 180.1443 - val_loss: 86781.7891 - val_mae: 186.2940\n",
      "Epoch 5/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 78941.2656 - mae: 178.3729\n",
      "Epoch 5: val_loss improved from 85150.42969 to 84595.46875, saving model to regressor_weights-05-84595.469.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 78941.2656 - mae: 178.3729 - val_loss: 84595.4688 - val_mae: 185.1752\n",
      "Epoch 6/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 79981.1641 - mae: 182.2885\n",
      "Epoch 6: val_loss improved from 84595.46875 to 82687.78906, saving model to regressor_weights-06-82687.789.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 78482.8750 - mae: 181.9034 - val_loss: 82687.7891 - val_mae: 175.8157\n",
      "Epoch 7/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 78234.9297 - mae: 175.8097\n",
      "Epoch 7: val_loss did not improve from 82687.78906\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 75824.0938 - mae: 175.3165 - val_loss: 103552.0469 - val_mae: 213.1767\n",
      "Epoch 8/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 74923.6250 - mae: 175.5081\n",
      "Epoch 8: val_loss did not improve from 82687.78906\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 74923.6250 - mae: 175.5081 - val_loss: 84574.2500 - val_mae: 180.5268\n",
      "Epoch 9/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 72191.5859 - mae: 168.9148\n",
      "Epoch 9: val_loss improved from 82687.78906 to 77789.17188, saving model to regressor_weights-09-77789.172.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 71530.8047 - mae: 169.1565 - val_loss: 77789.1719 - val_mae: 169.7033\n",
      "Epoch 10/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 70872.7031 - mae: 166.8143\n",
      "Epoch 10: val_loss improved from 77789.17188 to 77667.42969, saving model to regressor_weights-10-77667.430.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 70603.4062 - mae: 168.6423 - val_loss: 77667.4297 - val_mae: 172.3755\n",
      "Epoch 11/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 72958.1797 - mae: 173.2192\n",
      "Epoch 11: val_loss improved from 77667.42969 to 76288.79688, saving model to regressor_weights-11-76288.797.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 72958.1797 - mae: 173.2192 - val_loss: 76288.7969 - val_mae: 168.5740\n",
      "Epoch 12/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 74612.7578 - mae: 177.0351\n",
      "Epoch 12: val_loss did not improve from 76288.79688\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 74355.3984 - mae: 176.7285 - val_loss: 83019.6562 - val_mae: 180.9560\n",
      "Epoch 13/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 69868.0000 - mae: 168.9542\n",
      "Epoch 13: val_loss improved from 76288.79688 to 74600.02344, saving model to regressor_weights-13-74600.023.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 70284.0078 - mae: 168.6737 - val_loss: 74600.0234 - val_mae: 165.9403\n",
      "Epoch 14/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 68718.0234 - mae: 164.6417\n",
      "Epoch 14: val_loss improved from 74600.02344 to 72241.75000, saving model to regressor_weights-14-72241.750.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 66884.6719 - mae: 164.3043 - val_loss: 72241.7500 - val_mae: 166.2406\n",
      "Epoch 15/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 67046.7188 - mae: 163.9277\n",
      "Epoch 15: val_loss improved from 72241.75000 to 71819.93750, saving model to regressor_weights-15-71819.938.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 65671.3828 - mae: 162.5640 - val_loss: 71819.9375 - val_mae: 161.8786\n",
      "Epoch 16/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 66302.1953 - mae: 163.3277\n",
      "Epoch 16: val_loss improved from 71819.93750 to 71477.77344, saving model to regressor_weights-16-71477.773.hdf5\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 65705.5625 - mae: 162.9917 - val_loss: 71477.7734 - val_mae: 165.0567\n",
      "Epoch 17/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 70416.4141 - mae: 173.9807\n",
      "Epoch 17: val_loss did not improve from 71477.77344\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 68981.3438 - mae: 171.8394 - val_loss: 71626.0156 - val_mae: 170.4307\n",
      "Epoch 18/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 64947.4883 - mae: 162.7366\n",
      "Epoch 18: val_loss improved from 71477.77344 to 70259.39062, saving model to regressor_weights-18-70259.391.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 64039.8320 - mae: 162.2730 - val_loss: 70259.3906 - val_mae: 161.1354\n",
      "Epoch 19/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 59675.2344 - mae: 162.7821\n",
      "Epoch 19: val_loss did not improve from 70259.39062\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 63995.2344 - mae: 162.3914 - val_loss: 73111.5391 - val_mae: 172.5299\n",
      "Epoch 20/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 66236.8359 - mae: 167.6367\n",
      "Epoch 20: val_loss improved from 70259.39062 to 67537.35156, saving model to regressor_weights-20-67537.352.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 65368.4375 - mae: 166.8807 - val_loss: 67537.3516 - val_mae: 161.0040\n",
      "Epoch 21/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 63798.9766 - mae: 158.4251\n",
      "Epoch 21: val_loss did not improve from 67537.35156\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 61855.9609 - mae: 157.6233 - val_loss: 67968.9453 - val_mae: 160.5246\n",
      "Epoch 22/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 62659.7383 - mae: 162.9833\n",
      "Epoch 22: val_loss did not improve from 67537.35156\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 62144.1758 - mae: 162.2133 - val_loss: 69238.6719 - val_mae: 159.9018\n",
      "Epoch 23/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 62424.0703 - mae: 159.2478\n",
      "Epoch 23: val_loss improved from 67537.35156 to 67009.60938, saving model to regressor_weights-23-67009.609.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 61455.5273 - mae: 158.4510 - val_loss: 67009.6094 - val_mae: 163.6099\n",
      "Epoch 24/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 58783.1680 - mae: 156.1210\n",
      "Epoch 24: val_loss improved from 67009.60938 to 64633.08594, saving model to regressor_weights-24-64633.086.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 59278.0078 - mae: 156.4514 - val_loss: 64633.0859 - val_mae: 155.2231\n",
      "Epoch 25/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 57827.5156 - mae: 157.8262\n",
      "Epoch 25: val_loss improved from 64633.08594 to 64130.80469, saving model to regressor_weights-25-64130.805.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 59776.3867 - mae: 157.9673 - val_loss: 64130.8047 - val_mae: 159.8861\n",
      "Epoch 26/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 59330.1523 - mae: 156.3754\n",
      "Epoch 26: val_loss did not improve from 64130.80469\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 58880.7891 - mae: 155.7728 - val_loss: 64501.2852 - val_mae: 159.7588\n",
      "Epoch 27/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 51121.0508 - mae: 148.8115\n",
      "Epoch 27: val_loss did not improve from 64130.80469\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 57033.6484 - mae: 153.0820 - val_loss: 72671.1328 - val_mae: 180.3934\n",
      "Epoch 28/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 56427.3555 - mae: 154.0071\n",
      "Epoch 28: val_loss improved from 64130.80469 to 60796.83594, saving model to regressor_weights-28-60796.836.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 56375.6523 - mae: 154.0440 - val_loss: 60796.8359 - val_mae: 150.3739\n",
      "Epoch 29/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 54665.5508 - mae: 149.3056\n",
      "Epoch 29: val_loss improved from 60796.83594 to 60175.47656, saving model to regressor_weights-29-60175.477.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 54665.5508 - mae: 149.3056 - val_loss: 60175.4766 - val_mae: 154.5913\n",
      "Epoch 30/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 50355.2773 - mae: 146.3617\n",
      "Epoch 30: val_loss did not improve from 60175.47656\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 53212.5117 - mae: 148.0289 - val_loss: 60384.0117 - val_mae: 153.1849\n",
      "Epoch 31/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 54466.0039 - mae: 153.0396\n",
      "Epoch 31: val_loss did not improve from 60175.47656\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 53425.9336 - mae: 151.5669 - val_loss: 63216.4961 - val_mae: 153.0728\n",
      "Epoch 32/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 52546.9375 - mae: 145.4349\n",
      "Epoch 32: val_loss improved from 60175.47656 to 59070.60938, saving model to regressor_weights-32-59070.609.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 52546.9375 - mae: 145.4349 - val_loss: 59070.6094 - val_mae: 149.4662\n",
      "Epoch 33/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 48156.1992 - mae: 144.6771\n",
      "Epoch 33: val_loss improved from 59070.60938 to 57211.31641, saving model to regressor_weights-33-57211.316.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 52556.0117 - mae: 146.1620 - val_loss: 57211.3164 - val_mae: 151.7694\n",
      "Epoch 34/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 51732.9336 - mae: 145.4816\n",
      "Epoch 34: val_loss improved from 57211.31641 to 55916.24219, saving model to regressor_weights-34-55916.242.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 50740.8047 - mae: 145.6187 - val_loss: 55916.2422 - val_mae: 146.0040\n",
      "Epoch 35/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 45613.0000 - mae: 138.2377\n",
      "Epoch 35: val_loss improved from 55916.24219 to 55494.91016, saving model to regressor_weights-35-55494.910.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 49035.7266 - mae: 139.0995 - val_loss: 55494.9102 - val_mae: 150.3178\n",
      "Epoch 36/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 51106.6328 - mae: 145.1435\n",
      "Epoch 36: val_loss did not improve from 55494.91016\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 49701.8281 - mae: 144.4516 - val_loss: 55600.7422 - val_mae: 145.0783\n",
      "Epoch 37/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 49172.1875 - mae: 142.6829\n",
      "Epoch 37: val_loss did not improve from 55494.91016\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 49463.7031 - mae: 143.6192 - val_loss: 55847.9961 - val_mae: 151.4101\n",
      "Epoch 38/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 51332.1445 - mae: 147.4807\n",
      "Epoch 38: val_loss did not improve from 55494.91016\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 50718.6250 - mae: 147.3727 - val_loss: 59064.0117 - val_mae: 158.6001\n",
      "Epoch 39/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 49212.4609 - mae: 145.0085\n",
      "Epoch 39: val_loss improved from 55494.91016 to 51103.87891, saving model to regressor_weights-39-51103.879.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 48272.0234 - mae: 144.0294 - val_loss: 51103.8789 - val_mae: 140.4603\n",
      "Epoch 40/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 45084.2344 - mae: 135.6870\n",
      "Epoch 40: val_loss did not improve from 51103.87891\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 44882.0273 - mae: 135.1453 - val_loss: 52868.4922 - val_mae: 152.3154\n",
      "Epoch 41/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 46349.6406 - mae: 135.5886\n",
      "Epoch 41: val_loss improved from 51103.87891 to 51037.70312, saving model to regressor_weights-41-51037.703.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 45618.2930 - mae: 136.3604 - val_loss: 51037.7031 - val_mae: 140.6616\n",
      "Epoch 42/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 42993.8164 - mae: 135.8748\n",
      "Epoch 42: val_loss did not improve from 51037.70312\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 44436.5781 - mae: 137.0025 - val_loss: 63806.0312 - val_mae: 170.2654\n",
      "Epoch 43/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 44605.5000 - mae: 137.3495\n",
      "Epoch 43: val_loss improved from 51037.70312 to 49520.31641, saving model to regressor_weights-43-49520.316.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 44605.5000 - mae: 137.3495 - val_loss: 49520.3164 - val_mae: 148.0588\n",
      "Epoch 44/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 44663.8906 - mae: 135.9042\n",
      "Epoch 44: val_loss did not improve from 49520.31641\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 43578.8438 - mae: 135.0275 - val_loss: 50388.9766 - val_mae: 136.6909\n",
      "Epoch 45/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 44387.5000 - mae: 137.1801\n",
      "Epoch 45: val_loss improved from 49520.31641 to 49390.03906, saving model to regressor_weights-45-49390.039.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 42490.4492 - mae: 134.6917 - val_loss: 49390.0391 - val_mae: 139.6277\n",
      "Epoch 46/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 41390.5391 - mae: 132.2536\n",
      "Epoch 46: val_loss improved from 49390.03906 to 44587.40625, saving model to regressor_weights-46-44587.406.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 40769.9766 - mae: 131.4392 - val_loss: 44587.4062 - val_mae: 132.1325\n",
      "Epoch 47/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 40599.0039 - mae: 129.5781\n",
      "Epoch 47: val_loss did not improve from 44587.40625\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 40232.4570 - mae: 129.2470 - val_loss: 46573.2969 - val_mae: 134.2410\n",
      "Epoch 48/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 40216.9844 - mae: 132.1875\n",
      "Epoch 48: val_loss did not improve from 44587.40625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 40216.9844 - mae: 132.1875 - val_loss: 50573.8711 - val_mae: 146.5023\n",
      "Epoch 49/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 38998.5898 - mae: 132.4940\n",
      "Epoch 49: val_loss did not improve from 44587.40625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 40933.5508 - mae: 133.9998 - val_loss: 54137.5742 - val_mae: 149.8275\n",
      "Epoch 50/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 39882.6211 - mae: 127.8867\n",
      "Epoch 50: val_loss did not improve from 44587.40625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 39703.5977 - mae: 129.2709 - val_loss: 49777.6172 - val_mae: 151.7587\n",
      "Epoch 51/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 38528.8047 - mae: 126.4763\n",
      "Epoch 51: val_loss improved from 44587.40625 to 41769.60156, saving model to regressor_weights-51-41769.602.hdf5\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 38491.4297 - mae: 126.6937 - val_loss: 41769.6016 - val_mae: 128.0128\n",
      "Epoch 52/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 39566.9180 - mae: 131.4348\n",
      "Epoch 52: val_loss did not improve from 41769.60156\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 38737.5977 - mae: 130.2140 - val_loss: 42784.5820 - val_mae: 127.7155\n",
      "Epoch 53/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 37940.9688 - mae: 127.5742\n",
      "Epoch 53: val_loss improved from 41769.60156 to 41097.23438, saving model to regressor_weights-53-41097.234.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 37766.2422 - mae: 127.5187 - val_loss: 41097.2344 - val_mae: 125.1445\n",
      "Epoch 54/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 35629.4414 - mae: 121.8090\n",
      "Epoch 54: val_loss improved from 41097.23438 to 39291.15234, saving model to regressor_weights-54-39291.152.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 35629.4414 - mae: 121.8090 - val_loss: 39291.1523 - val_mae: 125.9417\n",
      "Epoch 55/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 34444.9062 - mae: 119.0653\n",
      "Epoch 55: val_loss did not improve from 39291.15234\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 34271.1289 - mae: 118.7541 - val_loss: 40653.0586 - val_mae: 125.8112\n",
      "Epoch 56/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 34845.2148 - mae: 121.5896\n",
      "Epoch 56: val_loss improved from 39291.15234 to 37378.65625, saving model to regressor_weights-56-37378.656.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 34722.1016 - mae: 121.4540 - val_loss: 37378.6562 - val_mae: 123.6144\n",
      "Epoch 57/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 35281.0625 - mae: 121.4261\n",
      "Epoch 57: val_loss did not improve from 37378.65625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 34396.6602 - mae: 120.7084 - val_loss: 37508.1094 - val_mae: 121.7487\n",
      "Epoch 58/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 32749.5215 - mae: 115.1392\n",
      "Epoch 58: val_loss did not improve from 37378.65625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 32203.4570 - mae: 115.7546 - val_loss: 37457.8008 - val_mae: 120.8661\n",
      "Epoch 59/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 34317.4961 - mae: 120.1430\n",
      "Epoch 59: val_loss did not improve from 37378.65625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 33868.3633 - mae: 120.2159 - val_loss: 38607.3789 - val_mae: 128.3577\n",
      "Epoch 60/350\n",
      "68/80 [========================>.....] - ETA: 0s - loss: 29602.7559 - mae: 119.6211\n",
      "Epoch 60: val_loss did not improve from 37378.65625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 33639.1406 - mae: 120.9017 - val_loss: 38004.1875 - val_mae: 123.8089\n",
      "Epoch 61/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 32640.0195 - mae: 118.3330\n",
      "Epoch 61: val_loss did not improve from 37378.65625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 31624.6816 - mae: 116.4993 - val_loss: 37844.2148 - val_mae: 127.0789\n",
      "Epoch 62/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 31937.6445 - mae: 115.1792\n",
      "Epoch 62: val_loss did not improve from 37378.65625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 32064.2012 - mae: 115.6470 - val_loss: 43584.9766 - val_mae: 140.4506\n",
      "Epoch 63/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 30119.0312 - mae: 114.1608\n",
      "Epoch 63: val_loss improved from 37378.65625 to 35711.28516, saving model to regressor_weights-63-35711.285.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 30465.6152 - mae: 114.1770 - val_loss: 35711.2852 - val_mae: 127.3563\n",
      "Epoch 64/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 27000.3281 - mae: 111.4388\n",
      "Epoch 64: val_loss improved from 35711.28516 to 34465.57031, saving model to regressor_weights-64-34465.570.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 29441.0801 - mae: 112.9539 - val_loss: 34465.5703 - val_mae: 117.4049\n",
      "Epoch 65/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 32996.3867 - mae: 121.8057\n",
      "Epoch 65: val_loss improved from 34465.57031 to 33742.07422, saving model to regressor_weights-65-33742.074.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 31821.1426 - mae: 120.0586 - val_loss: 33742.0742 - val_mae: 121.1355\n",
      "Epoch 66/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 27977.9316 - mae: 108.6691\n",
      "Epoch 66: val_loss did not improve from 33742.07422\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 28225.3984 - mae: 109.3800 - val_loss: 40226.7578 - val_mae: 144.7208\n",
      "Epoch 67/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 28864.4336 - mae: 111.8974\n",
      "Epoch 67: val_loss improved from 33742.07422 to 33400.12500, saving model to regressor_weights-67-33400.125.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 28671.8945 - mae: 111.7985 - val_loss: 33400.1250 - val_mae: 116.2635\n",
      "Epoch 68/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 27664.5195 - mae: 108.9792\n",
      "Epoch 68: val_loss improved from 33400.12500 to 30546.83398, saving model to regressor_weights-68-30546.834.hdf5\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 27376.2129 - mae: 108.7413 - val_loss: 30546.8340 - val_mae: 110.5134\n",
      "Epoch 69/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 27274.8945 - mae: 105.8152\n",
      "Epoch 69: val_loss did not improve from 30546.83398\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 26869.5781 - mae: 106.5619 - val_loss: 30632.1914 - val_mae: 109.4804\n",
      "Epoch 70/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 27508.7812 - mae: 109.9585\n",
      "Epoch 70: val_loss improved from 30546.83398 to 29602.58594, saving model to regressor_weights-70-29602.586.hdf5\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 27235.1758 - mae: 109.9097 - val_loss: 29602.5859 - val_mae: 111.6204\n",
      "Epoch 71/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 23434.8828 - mae: 101.8912\n",
      "Epoch 71: val_loss did not improve from 29602.58594\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 25236.1836 - mae: 102.5033 - val_loss: 31669.2891 - val_mae: 117.3014\n",
      "Epoch 72/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 26046.9277 - mae: 106.3442\n",
      "Epoch 72: val_loss did not improve from 29602.58594\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 25544.8066 - mae: 105.6255 - val_loss: 32045.2109 - val_mae: 113.6184\n",
      "Epoch 73/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 24180.3242 - mae: 99.7532\n",
      "Epoch 73: val_loss improved from 29602.58594 to 28971.80859, saving model to regressor_weights-73-28971.809.hdf5\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 24360.5312 - mae: 100.1673 - val_loss: 28971.8086 - val_mae: 108.6653\n",
      "Epoch 74/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 25032.4883 - mae: 102.3048\n",
      "Epoch 74: val_loss improved from 28971.80859 to 27943.34180, saving model to regressor_weights-74-27943.342.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 24588.1289 - mae: 102.8468 - val_loss: 27943.3418 - val_mae: 106.6486\n",
      "Epoch 75/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 24728.1992 - mae: 102.0479\n",
      "Epoch 75: val_loss did not improve from 27943.34180\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 24501.8105 - mae: 102.2744 - val_loss: 30692.7695 - val_mae: 112.6774\n",
      "Epoch 76/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 24670.8613 - mae: 101.8823\n",
      "Epoch 76: val_loss did not improve from 27943.34180\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 24392.4160 - mae: 102.2353 - val_loss: 40353.4180 - val_mae: 142.1416\n",
      "Epoch 77/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 22684.0137 - mae: 102.1543\n",
      "Epoch 77: val_loss did not improve from 27943.34180\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 24277.8965 - mae: 103.4819 - val_loss: 28057.7109 - val_mae: 113.3377\n",
      "Epoch 78/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 22703.7031 - mae: 98.7930 \n",
      "Epoch 78: val_loss did not improve from 27943.34180\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 21865.9844 - mae: 97.2604 - val_loss: 29270.0957 - val_mae: 109.9065\n",
      "Epoch 79/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 24961.7344 - mae: 105.1484\n",
      "Epoch 79: val_loss did not improve from 27943.34180\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 24360.2676 - mae: 104.4379 - val_loss: 28508.7383 - val_mae: 103.6913\n",
      "Epoch 80/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 22154.9043 - mae: 97.1527\n",
      "Epoch 80: val_loss did not improve from 27943.34180\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 22067.7500 - mae: 97.5834 - val_loss: 29065.0996 - val_mae: 116.3375\n",
      "Epoch 81/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 21679.2480 - mae: 97.1513\n",
      "Epoch 81: val_loss improved from 27943.34180 to 26783.65234, saving model to regressor_weights-81-26783.652.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 21438.3125 - mae: 96.7243 - val_loss: 26783.6523 - val_mae: 100.1353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 21694.6523 - mae: 97.1942\n",
      "Epoch 82: val_loss improved from 26783.65234 to 25962.27734, saving model to regressor_weights-82-25962.277.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 21649.7188 - mae: 97.1768 - val_loss: 25962.2773 - val_mae: 102.5116\n",
      "Epoch 83/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 21581.5938 - mae: 97.3926\n",
      "Epoch 83: val_loss did not improve from 25962.27734\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 20900.3867 - mae: 96.0766 - val_loss: 25980.2793 - val_mae: 108.6397\n",
      "Epoch 84/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 18209.1348 - mae: 88.9792\n",
      "Epoch 84: val_loss improved from 25962.27734 to 23760.43750, saving model to regressor_weights-84-23760.438.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 19500.0098 - mae: 89.9133 - val_loss: 23760.4375 - val_mae: 100.7198\n",
      "Epoch 85/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 21097.8281 - mae: 95.3740\n",
      "Epoch 85: val_loss did not improve from 23760.43750\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 20369.9805 - mae: 94.5012 - val_loss: 27073.0605 - val_mae: 104.5292\n",
      "Epoch 86/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 19849.6699 - mae: 92.1008\n",
      "Epoch 86: val_loss did not improve from 23760.43750\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 19334.7227 - mae: 91.5152 - val_loss: 25395.2227 - val_mae: 100.9275\n",
      "Epoch 87/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 22227.8516 - mae: 100.2351\n",
      "Epoch 87: val_loss did not improve from 23760.43750\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 22480.8203 - mae: 101.4962 - val_loss: 25159.5684 - val_mae: 106.3865\n",
      "Epoch 88/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 19569.9766 - mae: 92.4099\n",
      "Epoch 88: val_loss did not improve from 23760.43750\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 18794.4629 - mae: 90.9835 - val_loss: 24465.6523 - val_mae: 97.1110\n",
      "Epoch 89/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 18728.0703 - mae: 92.8806\n",
      "Epoch 89: val_loss improved from 23760.43750 to 22440.09375, saving model to regressor_weights-89-22440.094.hdf5\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 19673.1504 - mae: 92.7391 - val_loss: 22440.0938 - val_mae: 99.0639\n",
      "Epoch 90/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 20098.2227 - mae: 93.0514\n",
      "Epoch 90: val_loss did not improve from 22440.09375\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 19624.4258 - mae: 92.8096 - val_loss: 24077.7949 - val_mae: 107.4695\n",
      "Epoch 91/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 17985.1328 - mae: 88.0308\n",
      "Epoch 91: val_loss improved from 22440.09375 to 21445.29883, saving model to regressor_weights-91-21445.299.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 17990.2422 - mae: 88.7073 - val_loss: 21445.2988 - val_mae: 90.8661\n",
      "Epoch 92/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 18041.9629 - mae: 88.1556\n",
      "Epoch 92: val_loss did not improve from 21445.29883\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 18054.1680 - mae: 89.2032 - val_loss: 23727.3145 - val_mae: 98.5058\n",
      "Epoch 93/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 18968.4375 - mae: 92.9919\n",
      "Epoch 93: val_loss did not improve from 21445.29883\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 19160.7695 - mae: 93.1055 - val_loss: 22581.1953 - val_mae: 102.1993\n",
      "Epoch 94/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 17767.8594 - mae: 86.2780\n",
      "Epoch 94: val_loss improved from 21445.29883 to 19943.82422, saving model to regressor_weights-94-19943.824.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 17087.8496 - mae: 85.2673 - val_loss: 19943.8242 - val_mae: 88.9435\n",
      "Epoch 95/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 17927.3223 - mae: 88.7034\n",
      "Epoch 95: val_loss did not improve from 19943.82422\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 17880.4922 - mae: 89.4112 - val_loss: 22450.0117 - val_mae: 93.5237\n",
      "Epoch 96/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 15673.6895 - mae: 83.5798\n",
      "Epoch 96: val_loss did not improve from 19943.82422\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 16528.8418 - mae: 83.9219 - val_loss: 20874.6641 - val_mae: 97.9785\n",
      "Epoch 97/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 14538.2588 - mae: 81.1702\n",
      "Epoch 97: val_loss improved from 19943.82422 to 18113.40625, saving model to regressor_weights-97-18113.406.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 15812.5010 - mae: 83.4228 - val_loss: 18113.4062 - val_mae: 87.0239\n",
      "Epoch 98/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 15794.2256 - mae: 83.8080\n",
      "Epoch 98: val_loss did not improve from 18113.40625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 15571.8154 - mae: 83.4928 - val_loss: 22035.4043 - val_mae: 90.9299\n",
      "Epoch 99/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 15979.6914 - mae: 84.0497\n",
      "Epoch 99: val_loss did not improve from 18113.40625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 15870.6953 - mae: 84.0451 - val_loss: 18400.8809 - val_mae: 84.9484\n",
      "Epoch 100/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 15002.8828 - mae: 78.7864\n",
      "Epoch 100: val_loss did not improve from 18113.40625\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 14825.1670 - mae: 78.9279 - val_loss: 19257.3320 - val_mae: 93.1779\n",
      "Epoch 101/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 15415.9004 - mae: 82.3683\n",
      "Epoch 101: val_loss did not improve from 18113.40625\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 15158.4902 - mae: 81.7898 - val_loss: 18641.0820 - val_mae: 90.5377\n",
      "Epoch 102/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 15455.6592 - mae: 82.3160\n",
      "Epoch 102: val_loss did not improve from 18113.40625\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 15455.6592 - mae: 82.3160 - val_loss: 20893.6250 - val_mae: 102.4535\n",
      "Epoch 103/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 14104.6836 - mae: 77.7212\n",
      "Epoch 103: val_loss improved from 18113.40625 to 18046.66797, saving model to regressor_weights-103-18046.668.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 14052.5596 - mae: 77.8042 - val_loss: 18046.6680 - val_mae: 88.5354\n",
      "Epoch 104/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 13846.3076 - mae: 77.5145\n",
      "Epoch 104: val_loss did not improve from 18046.66797\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 13810.0479 - mae: 77.4373 - val_loss: 18150.2246 - val_mae: 91.0318\n",
      "Epoch 105/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 14333.9414 - mae: 80.1710\n",
      "Epoch 105: val_loss improved from 18046.66797 to 16646.83984, saving model to regressor_weights-105-16646.840.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 14077.2666 - mae: 79.8074 - val_loss: 16646.8398 - val_mae: 83.4347\n",
      "Epoch 106/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 13957.8984 - mae: 77.4109\n",
      "Epoch 106: val_loss did not improve from 16646.83984\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 13814.4014 - mae: 77.6235 - val_loss: 17032.8965 - val_mae: 88.2494\n",
      "Epoch 107/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 13237.3047 - mae: 75.4803\n",
      "Epoch 107: val_loss improved from 16646.83984 to 16391.48047, saving model to regressor_weights-107-16391.480.hdf5\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 13140.6846 - mae: 75.3006 - val_loss: 16391.4805 - val_mae: 86.1338\n",
      "Epoch 108/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 12786.3779 - mae: 74.9160\n",
      "Epoch 108: val_loss improved from 16391.48047 to 15513.94434, saving model to regressor_weights-108-15513.944.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 12758.5537 - mae: 75.0651 - val_loss: 15513.9443 - val_mae: 77.6651\n",
      "Epoch 109/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 14274.5430 - mae: 81.8467\n",
      "Epoch 109: val_loss did not improve from 15513.94434\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 14147.4482 - mae: 81.7555 - val_loss: 16233.7324 - val_mae: 85.2244\n",
      "Epoch 110/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 12829.4736 - mae: 74.9399\n",
      "Epoch 110: val_loss did not improve from 15513.94434\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 12877.4268 - mae: 75.5250 - val_loss: 15647.2529 - val_mae: 80.7373\n",
      "Epoch 111/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 12449.8096 - mae: 74.5034\n",
      "Epoch 111: val_loss improved from 15513.94434 to 14951.35449, saving model to regressor_weights-111-14951.354.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 12529.3135 - mae: 74.4971 - val_loss: 14951.3545 - val_mae: 78.4301\n",
      "Epoch 112/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 12045.5078 - mae: 71.8430\n",
      "Epoch 112: val_loss did not improve from 14951.35449\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 12000.2520 - mae: 72.2228 - val_loss: 18106.1113 - val_mae: 90.4650\n",
      "Epoch 113/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 11775.8223 - mae: 70.8654\n",
      "Epoch 113: val_loss did not improve from 14951.35449\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 11637.1846 - mae: 71.3537 - val_loss: 18621.3262 - val_mae: 101.4959\n",
      "Epoch 114/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 13537.2041 - mae: 80.4310\n",
      "Epoch 114: val_loss improved from 14951.35449 to 14774.61328, saving model to regressor_weights-114-14774.613.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 13375.4072 - mae: 79.8694 - val_loss: 14774.6133 - val_mae: 79.7950\n",
      "Epoch 115/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 11488.1846 - mae: 70.2953\n",
      "Epoch 115: val_loss improved from 14774.61328 to 14285.47949, saving model to regressor_weights-115-14285.479.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 11295.2275 - mae: 70.4310 - val_loss: 14285.4795 - val_mae: 74.8508\n",
      "Epoch 116/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 11287.3711 - mae: 71.7956\n",
      "Epoch 116: val_loss did not improve from 14285.47949\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 11902.1064 - mae: 73.6035 - val_loss: 19612.7461 - val_mae: 98.2696\n",
      "Epoch 117/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 12690.5547 - mae: 77.5148\n",
      "Epoch 117: val_loss improved from 14285.47949 to 12593.24121, saving model to regressor_weights-117-12593.241.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 12757.3105 - mae: 77.1737 - val_loss: 12593.2412 - val_mae: 73.9296\n",
      "Epoch 118/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 10968.2002 - mae: 69.9407\n",
      "Epoch 118: val_loss did not improve from 12593.24121\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 10934.4248 - mae: 70.1012 - val_loss: 13009.9082 - val_mae: 73.4415\n",
      "Epoch 119/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 10374.8955 - mae: 68.9926\n",
      "Epoch 119: val_loss did not improve from 12593.24121\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 10741.0000 - mae: 69.6509 - val_loss: 15815.4160 - val_mae: 84.5406\n",
      "Epoch 120/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 11443.3936 - mae: 72.1558\n",
      "Epoch 120: val_loss did not improve from 12593.24121\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 11355.6426 - mae: 72.1583 - val_loss: 13597.8740 - val_mae: 76.6830\n",
      "Epoch 121/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 9934.3896 - mae: 65.2931\n",
      "Epoch 121: val_loss improved from 12593.24121 to 12177.25977, saving model to regressor_weights-121-12177.260.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 9933.3799 - mae: 65.8427 - val_loss: 12177.2598 - val_mae: 72.8449\n",
      "Epoch 122/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 9785.2715 - mae: 66.5026\n",
      "Epoch 122: val_loss did not improve from 12177.25977\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 9822.9834 - mae: 66.7494 - val_loss: 14964.4590 - val_mae: 80.4405\n",
      "Epoch 123/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 9600.2617 - mae: 65.2422\n",
      "Epoch 123: val_loss did not improve from 12177.25977\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 9674.3584 - mae: 65.7259 - val_loss: 12430.2051 - val_mae: 71.7505\n",
      "Epoch 124/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 9483.1436 - mae: 66.2210\n",
      "Epoch 124: val_loss improved from 12177.25977 to 11764.79297, saving model to regressor_weights-124-11764.793.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9639.9551 - mae: 66.4442 - val_loss: 11764.7930 - val_mae: 73.6221\n",
      "Epoch 125/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 9204.7891 - mae: 65.5553\n",
      "Epoch 125: val_loss did not improve from 11764.79297\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 9455.8594 - mae: 65.6869 - val_loss: 12432.2646 - val_mae: 71.4669\n",
      "Epoch 126/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 10705.2031 - mae: 71.1791\n",
      "Epoch 126: val_loss did not improve from 11764.79297\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 10375.6826 - mae: 70.2125 - val_loss: 13268.9922 - val_mae: 71.5710\n",
      "Epoch 127/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 9348.0088 - mae: 65.3578\n",
      "Epoch 127: val_loss did not improve from 11764.79297\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 9435.2471 - mae: 66.0371 - val_loss: 13472.0391 - val_mae: 81.5763\n",
      "Epoch 128/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 9731.4023 - mae: 68.1327\n",
      "Epoch 128: val_loss improved from 11764.79297 to 11357.42871, saving model to regressor_weights-128-11357.429.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9708.6631 - mae: 68.3920 - val_loss: 11357.4287 - val_mae: 72.4896\n",
      "Epoch 129/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 10051.0361 - mae: 69.0240\n",
      "Epoch 129: val_loss did not improve from 11357.42871\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 9938.8037 - mae: 69.2894 - val_loss: 12015.7686 - val_mae: 67.0166\n",
      "Epoch 130/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 9030.0283 - mae: 63.8336\n",
      "Epoch 130: val_loss improved from 11357.42871 to 10430.10254, saving model to regressor_weights-130-10430.103.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8916.1680 - mae: 63.4676 - val_loss: 10430.1025 - val_mae: 62.9999\n",
      "Epoch 131/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 8770.0879 - mae: 64.1165\n",
      "Epoch 131: val_loss did not improve from 10430.10254\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 8626.0586 - mae: 63.4932 - val_loss: 11469.9600 - val_mae: 75.6631\n",
      "Epoch 132/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 8716.1846 - mae: 63.6110\n",
      "Epoch 132: val_loss did not improve from 10430.10254\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8641.8340 - mae: 63.6175 - val_loss: 14715.6104 - val_mae: 83.0952\n",
      "Epoch 133/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 10031.7979 - mae: 70.5395\n",
      "Epoch 133: val_loss did not improve from 10430.10254\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 9954.0576 - mae: 70.3388 - val_loss: 15825.4424 - val_mae: 87.4919\n",
      "Epoch 134/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 8729.1689 - mae: 65.3403\n",
      "Epoch 134: val_loss improved from 10430.10254 to 9471.70117, saving model to regressor_weights-134-9471.701.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8957.6670 - mae: 65.1982 - val_loss: 9471.7012 - val_mae: 66.7023\n",
      "Epoch 135/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 7739.0684 - mae: 61.2297\n",
      "Epoch 135: val_loss did not improve from 9471.70117\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 7984.4360 - mae: 61.6010 - val_loss: 14160.2148 - val_mae: 93.1612\n",
      "Epoch 136/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 8359.5811 - mae: 62.1127\n",
      "Epoch 136: val_loss did not improve from 9471.70117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 6ms/step - loss: 8443.0723 - mae: 63.1142 - val_loss: 9494.5049 - val_mae: 62.6239\n",
      "Epoch 137/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 8419.2520 - mae: 63.3608\n",
      "Epoch 137: val_loss did not improve from 9471.70117\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 8126.2402 - mae: 62.4721 - val_loss: 10439.9131 - val_mae: 73.1618\n",
      "Epoch 138/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 7189.4434 - mae: 58.8826\n",
      "Epoch 138: val_loss did not improve from 9471.70117\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7530.9258 - mae: 59.5182 - val_loss: 10436.8252 - val_mae: 70.2133\n",
      "Epoch 139/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 8705.6953 - mae: 64.5773\n",
      "Epoch 139: val_loss improved from 9471.70117 to 9040.08691, saving model to regressor_weights-139-9040.087.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8304.8623 - mae: 63.3846 - val_loss: 9040.0869 - val_mae: 60.6733\n",
      "Epoch 140/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 7181.1958 - mae: 57.8934\n",
      "Epoch 140: val_loss did not improve from 9040.08691\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 7217.7427 - mae: 58.0148 - val_loss: 9077.2988 - val_mae: 60.1694\n",
      "Epoch 141/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 7187.7622 - mae: 57.3814\n",
      "Epoch 141: val_loss did not improve from 9040.08691\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7122.3101 - mae: 57.1395 - val_loss: 9419.3594 - val_mae: 60.3840\n",
      "Epoch 142/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 6990.4053 - mae: 57.8412\n",
      "Epoch 142: val_loss improved from 9040.08691 to 8968.68359, saving model to regressor_weights-142-8968.684.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7109.7583 - mae: 57.9478 - val_loss: 8968.6836 - val_mae: 63.6573\n",
      "Epoch 143/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 7286.9043 - mae: 59.3462\n",
      "Epoch 143: val_loss improved from 8968.68359 to 8058.03662, saving model to regressor_weights-143-8058.037.hdf5\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 7297.2925 - mae: 59.5085 - val_loss: 8058.0366 - val_mae: 57.6695\n",
      "Epoch 144/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 7105.8311 - mae: 58.9464\n",
      "Epoch 144: val_loss improved from 8058.03662 to 7935.68506, saving model to regressor_weights-144-7935.685.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 6977.5981 - mae: 58.4788 - val_loss: 7935.6851 - val_mae: 57.9420\n",
      "Epoch 145/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 7653.6377 - mae: 62.0214\n",
      "Epoch 145: val_loss did not improve from 7935.68506\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 7628.0464 - mae: 62.0384 - val_loss: 13015.5410 - val_mae: 83.9599\n",
      "Epoch 146/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 6514.8232 - mae: 55.4223\n",
      "Epoch 146: val_loss did not improve from 7935.68506\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 6615.0107 - mae: 55.7910 - val_loss: 8962.5303 - val_mae: 61.1287\n",
      "Epoch 147/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 6480.7681 - mae: 55.8207\n",
      "Epoch 147: val_loss improved from 7935.68506 to 7850.60498, saving model to regressor_weights-147-7850.605.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 6408.4526 - mae: 55.0045 - val_loss: 7850.6050 - val_mae: 54.1756\n",
      "Epoch 148/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 6231.2881 - mae: 53.9122\n",
      "Epoch 148: val_loss improved from 7850.60498 to 7751.21680, saving model to regressor_weights-148-7751.217.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6222.1748 - mae: 53.9015 - val_loss: 7751.2168 - val_mae: 54.9803\n",
      "Epoch 149/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 6143.3062 - mae: 54.0797\n",
      "Epoch 149: val_loss improved from 7751.21680 to 7271.13574, saving model to regressor_weights-149-7271.136.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6080.7017 - mae: 53.7887 - val_loss: 7271.1357 - val_mae: 57.7265\n",
      "Epoch 150/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 6403.1421 - mae: 55.9653\n",
      "Epoch 150: val_loss did not improve from 7271.13574\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 6542.1001 - mae: 56.1166 - val_loss: 8243.9639 - val_mae: 60.3656\n",
      "Epoch 151/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 5523.1836 - mae: 51.7893\n",
      "Epoch 151: val_loss did not improve from 7271.13574\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 5876.0308 - mae: 52.5476 - val_loss: 7570.1113 - val_mae: 59.2664\n",
      "Epoch 152/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 6276.3721 - mae: 54.2859\n",
      "Epoch 152: val_loss improved from 7271.13574 to 6959.09717, saving model to regressor_weights-152-6959.097.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6101.4849 - mae: 53.7858 - val_loss: 6959.0972 - val_mae: 52.5045\n",
      "Epoch 153/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 5936.4912 - mae: 53.9181\n",
      "Epoch 153: val_loss did not improve from 6959.09717\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 5848.7080 - mae: 53.7084 - val_loss: 9237.9971 - val_mae: 61.6113\n",
      "Epoch 154/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 6937.5981 - mae: 58.9348\n",
      "Epoch 154: val_loss did not improve from 6959.09717\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6905.2715 - mae: 58.7799 - val_loss: 7159.2778 - val_mae: 53.4091\n",
      "Epoch 155/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 5212.2363 - mae: 49.8923\n",
      "Epoch 155: val_loss did not improve from 6959.09717\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 5205.1616 - mae: 50.0317 - val_loss: 7609.1367 - val_mae: 56.5391\n",
      "Epoch 156/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 5316.8403 - mae: 49.9963\n",
      "Epoch 156: val_loss did not improve from 6959.09717\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 5316.8403 - mae: 49.9963 - val_loss: 7523.2573 - val_mae: 57.3337\n",
      "Epoch 157/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 5125.0303 - mae: 48.0567\n",
      "Epoch 157: val_loss did not improve from 6959.09717\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 5051.3257 - mae: 47.7979 - val_loss: 8156.5288 - val_mae: 64.9167\n",
      "Epoch 158/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 5476.1035 - mae: 50.6776\n",
      "Epoch 158: val_loss did not improve from 6959.09717\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5403.4297 - mae: 50.3860 - val_loss: 8988.5469 - val_mae: 72.9170\n",
      "Epoch 159/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 5165.9502 - mae: 48.7143\n",
      "Epoch 159: val_loss improved from 6959.09717 to 6320.23096, saving model to regressor_weights-159-6320.231.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5017.0200 - mae: 48.0314 - val_loss: 6320.2310 - val_mae: 51.8063\n",
      "Epoch 160/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 5138.1831 - mae: 50.0769\n",
      "Epoch 160: val_loss did not improve from 6320.23096\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 5068.9873 - mae: 49.9691 - val_loss: 6495.4795 - val_mae: 50.5248\n",
      "Epoch 161/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 5552.8848 - mae: 52.6055\n",
      "Epoch 161: val_loss did not improve from 6320.23096\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5502.5288 - mae: 52.1805 - val_loss: 6398.7827 - val_mae: 54.7753\n",
      "Epoch 162/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 4993.9082 - mae: 48.3013\n",
      "Epoch 162: val_loss did not improve from 6320.23096\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 4862.9595 - mae: 47.6784 - val_loss: 6788.3003 - val_mae: 52.2975\n",
      "Epoch 163/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 4794.0083 - mae: 46.7644\n",
      "Epoch 163: val_loss improved from 6320.23096 to 6080.71924, saving model to regressor_weights-163-6080.719.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4775.0718 - mae: 46.6714 - val_loss: 6080.7192 - val_mae: 49.4160\n",
      "Epoch 164/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 4563.0840 - mae: 46.2959\n",
      "Epoch 164: val_loss did not improve from 6080.71924\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 4550.0410 - mae: 46.2611 - val_loss: 6434.7173 - val_mae: 51.5376\n",
      "Epoch 165/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 4369.2373 - mae: 45.6274\n",
      "Epoch 165: val_loss did not improve from 6080.71924\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4542.1182 - mae: 46.6905 - val_loss: 6317.1621 - val_mae: 50.7033\n",
      "Epoch 166/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 4395.9673 - mae: 46.4174\n",
      "Epoch 166: val_loss did not improve from 6080.71924\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 4425.1235 - mae: 46.2987 - val_loss: 6410.4800 - val_mae: 48.7216\n",
      "Epoch 167/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 4964.2310 - mae: 49.9722\n",
      "Epoch 167: val_loss improved from 6080.71924 to 5660.19238, saving model to regressor_weights-167-5660.192.hdf5\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 4963.9404 - mae: 50.2678 - val_loss: 5660.1924 - val_mae: 50.3449\n",
      "Epoch 168/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 4738.7520 - mae: 48.0018\n",
      "Epoch 168: val_loss improved from 5660.19238 to 5310.00488, saving model to regressor_weights-168-5310.005.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4738.7520 - mae: 48.0018 - val_loss: 5310.0049 - val_mae: 47.6777\n",
      "Epoch 169/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 4944.8149 - mae: 49.3633\n",
      "Epoch 169: val_loss did not improve from 5310.00488\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 4912.0581 - mae: 49.1528 - val_loss: 7744.2495 - val_mae: 63.1517\n",
      "Epoch 170/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 4296.3115 - mae: 45.3112\n",
      "Epoch 170: val_loss improved from 5310.00488 to 5289.45410, saving model to regressor_weights-170-5289.454.hdf5\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 4291.6094 - mae: 45.3898 - val_loss: 5289.4541 - val_mae: 48.0103\n",
      "Epoch 171/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 4073.1028 - mae: 43.5556\n",
      "Epoch 171: val_loss improved from 5289.45410 to 5207.59814, saving model to regressor_weights-171-5207.598.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4073.1028 - mae: 43.5556 - val_loss: 5207.5981 - val_mae: 46.2112\n",
      "Epoch 172/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 3944.1892 - mae: 43.8913\n",
      "Epoch 172: val_loss did not improve from 5207.59814\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 3977.1399 - mae: 43.9718 - val_loss: 5573.1401 - val_mae: 45.2715\n",
      "Epoch 173/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 3719.5532 - mae: 41.7997\n",
      "Epoch 173: val_loss did not improve from 5207.59814\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3788.5237 - mae: 42.2327 - val_loss: 8723.7998 - val_mae: 67.9103\n",
      "Epoch 174/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 4026.0381 - mae: 44.0055\n",
      "Epoch 174: val_loss improved from 5207.59814 to 4914.49463, saving model to regressor_weights-174-4914.495.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4038.6387 - mae: 44.1804 - val_loss: 4914.4946 - val_mae: 47.0734\n",
      "Epoch 175/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 4239.0259 - mae: 45.3733\n",
      "Epoch 175: val_loss did not improve from 4914.49463\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 4118.7388 - mae: 44.7654 - val_loss: 5591.4126 - val_mae: 47.6058\n",
      "Epoch 176/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 3859.3303 - mae: 43.8169\n",
      "Epoch 176: val_loss did not improve from 4914.49463\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3954.8145 - mae: 43.9502 - val_loss: 5507.6353 - val_mae: 49.1970\n",
      "Epoch 177/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 3607.5591 - mae: 40.9067\n",
      "Epoch 177: val_loss improved from 4914.49463 to 4661.64307, saving model to regressor_weights-177-4661.643.hdf5\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3597.7078 - mae: 40.5722 - val_loss: 4661.6431 - val_mae: 41.9144\n",
      "Epoch 178/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 4085.1284 - mae: 44.2894\n",
      "Epoch 178: val_loss did not improve from 4661.64307\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4016.3757 - mae: 44.0191 - val_loss: 5605.7954 - val_mae: 47.3167\n",
      "Epoch 179/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 3993.7844 - mae: 44.9238\n",
      "Epoch 179: val_loss did not improve from 4661.64307\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4031.1648 - mae: 44.9708 - val_loss: 5402.2407 - val_mae: 53.4220\n",
      "Epoch 180/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 3830.6377 - mae: 43.0418\n",
      "Epoch 180: val_loss did not improve from 4661.64307\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3805.2390 - mae: 42.9853 - val_loss: 5232.9907 - val_mae: 47.7159\n",
      "Epoch 181/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 3457.7893 - mae: 41.0394\n",
      "Epoch 181: val_loss did not improve from 4661.64307\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3545.6514 - mae: 41.4657 - val_loss: 5141.5864 - val_mae: 44.6625\n",
      "Epoch 182/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 3549.5911 - mae: 41.5805\n",
      "Epoch 182: val_loss did not improve from 4661.64307\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3595.9080 - mae: 41.7983 - val_loss: 6810.6094 - val_mae: 49.6103\n",
      "Epoch 183/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 3337.5295 - mae: 40.8392\n",
      "Epoch 183: val_loss did not improve from 4661.64307\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3419.3613 - mae: 40.9558 - val_loss: 5451.4756 - val_mae: 47.5943\n",
      "Epoch 184/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 3460.3572 - mae: 40.9397\n",
      "Epoch 184: val_loss did not improve from 4661.64307\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3545.9214 - mae: 41.4330 - val_loss: 5185.5762 - val_mae: 48.9029\n",
      "Epoch 185/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 3264.0635 - mae: 40.2269\n",
      "Epoch 185: val_loss did not improve from 4661.64307\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3495.2083 - mae: 41.4727 - val_loss: 5392.1904 - val_mae: 50.8788\n",
      "Epoch 186/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 3846.2808 - mae: 43.2888\n",
      "Epoch 186: val_loss improved from 4661.64307 to 3881.43945, saving model to regressor_weights-186-3881.439.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3794.1411 - mae: 43.3923 - val_loss: 3881.4395 - val_mae: 39.1128\n",
      "Epoch 187/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 3066.5095 - mae: 37.7640\n",
      "Epoch 187: val_loss did not improve from 3881.43945\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3066.5095 - mae: 37.7640 - val_loss: 4537.8813 - val_mae: 42.5737\n",
      "Epoch 188/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 2937.3118 - mae: 36.9956\n",
      "Epoch 188: val_loss did not improve from 3881.43945\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2908.1321 - mae: 37.0143 - val_loss: 4271.4751 - val_mae: 39.2825\n",
      "Epoch 189/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 3488.8118 - mae: 42.0220\n",
      "Epoch 189: val_loss did not improve from 3881.43945\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3508.8381 - mae: 41.8978 - val_loss: 4131.4336 - val_mae: 39.8121\n",
      "Epoch 190/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 2834.2371 - mae: 37.2396\n",
      "Epoch 190: val_loss improved from 3881.43945 to 3754.92578, saving model to regressor_weights-190-3754.926.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2919.2927 - mae: 37.3208 - val_loss: 3754.9258 - val_mae: 39.1589\n",
      "Epoch 191/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 2806.7212 - mae: 36.5709\n",
      "Epoch 191: val_loss did not improve from 3754.92578\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2756.4299 - mae: 35.9369 - val_loss: 4184.3970 - val_mae: 41.0970\n",
      "Epoch 192/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/80 [==========================>...] - ETA: 0s - loss: 2983.5571 - mae: 38.0300\n",
      "Epoch 192: val_loss improved from 3754.92578 to 3688.52637, saving model to regressor_weights-192-3688.526.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2962.7720 - mae: 38.0442 - val_loss: 3688.5264 - val_mae: 36.8771\n",
      "Epoch 193/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 3138.6204 - mae: 38.6884\n",
      "Epoch 193: val_loss did not improve from 3688.52637\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3388.8494 - mae: 40.5930 - val_loss: 5380.4185 - val_mae: 50.6281\n",
      "Epoch 194/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 3819.6335 - mae: 43.5376\n",
      "Epoch 194: val_loss did not improve from 3688.52637\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3769.5417 - mae: 43.1534 - val_loss: 5023.4302 - val_mae: 42.7295\n",
      "Epoch 195/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 3046.8550 - mae: 37.4288\n",
      "Epoch 195: val_loss improved from 3688.52637 to 3454.97070, saving model to regressor_weights-195-3454.971.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 2982.1729 - mae: 37.1199 - val_loss: 3454.9707 - val_mae: 36.3142\n",
      "Epoch 196/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 2462.3616 - mae: 33.3870\n",
      "Epoch 196: val_loss did not improve from 3454.97070\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2483.6382 - mae: 33.5971 - val_loss: 3746.5615 - val_mae: 38.1061\n",
      "Epoch 197/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 2657.5786 - mae: 36.0960\n",
      "Epoch 197: val_loss did not improve from 3454.97070\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2625.8337 - mae: 36.0773 - val_loss: 4898.0889 - val_mae: 43.2683\n",
      "Epoch 198/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 3855.1685 - mae: 45.4006\n",
      "Epoch 198: val_loss did not improve from 3454.97070\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3724.2852 - mae: 44.5782 - val_loss: 4349.7964 - val_mae: 46.8063\n",
      "Epoch 199/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 2930.8875 - mae: 37.6994\n",
      "Epoch 199: val_loss did not improve from 3454.97070\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3027.8015 - mae: 38.0918 - val_loss: 4628.2505 - val_mae: 47.2891\n",
      "Epoch 200/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 6346.7915 - mae: 55.8903\n",
      "Epoch 200: val_loss did not improve from 3454.97070\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 6040.2896 - mae: 54.0243 - val_loss: 4308.9985 - val_mae: 44.3341\n",
      "Epoch 201/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 2735.2310 - mae: 36.4830\n",
      "Epoch 201: val_loss did not improve from 3454.97070\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2747.0151 - mae: 36.8902 - val_loss: 3916.2097 - val_mae: 39.7325\n",
      "Epoch 202/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 3097.0774 - mae: 38.0882\n",
      "Epoch 202: val_loss did not improve from 3454.97070\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3055.3000 - mae: 37.8910 - val_loss: 4301.1250 - val_mae: 44.5302\n",
      "Epoch 203/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 2617.4729 - mae: 35.2236\n",
      "Epoch 203: val_loss improved from 3454.97070 to 3111.08423, saving model to regressor_weights-203-3111.084.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2617.4729 - mae: 35.2236 - val_loss: 3111.0842 - val_mae: 37.2758\n",
      "Epoch 204/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 2269.7827 - mae: 32.7548\n",
      "Epoch 204: val_loss improved from 3111.08423 to 2988.68555, saving model to regressor_weights-204-2988.686.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2270.1406 - mae: 32.9029 - val_loss: 2988.6855 - val_mae: 34.0675\n",
      "Epoch 205/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 2539.7363 - mae: 34.7978\n",
      "Epoch 205: val_loss did not improve from 2988.68555\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2553.1089 - mae: 35.0231 - val_loss: 3571.8066 - val_mae: 37.9526\n",
      "Epoch 206/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 2300.2185 - mae: 33.3083\n",
      "Epoch 206: val_loss did not improve from 2988.68555\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 2352.0715 - mae: 33.1766 - val_loss: 3264.5273 - val_mae: 35.4158\n",
      "Epoch 207/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 2309.2085 - mae: 32.2397\n",
      "Epoch 207: val_loss did not improve from 2988.68555\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2269.8157 - mae: 32.2021 - val_loss: 3088.2354 - val_mae: 33.3363\n",
      "Epoch 208/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2222.1619 - mae: 32.1442\n",
      "Epoch 208: val_loss improved from 2988.68555 to 2819.05811, saving model to regressor_weights-208-2819.058.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2236.6575 - mae: 32.2835 - val_loss: 2819.0581 - val_mae: 34.0478\n",
      "Epoch 209/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 2391.7913 - mae: 33.4476\n",
      "Epoch 209: val_loss did not improve from 2819.05811\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2347.2061 - mae: 33.3066 - val_loss: 3028.4702 - val_mae: 36.0186\n",
      "Epoch 210/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 2322.4143 - mae: 33.2864\n",
      "Epoch 210: val_loss did not improve from 2819.05811\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2271.4324 - mae: 33.0474 - val_loss: 2939.5793 - val_mae: 33.7234\n",
      "Epoch 211/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 2280.0654 - mae: 32.1215\n",
      "Epoch 211: val_loss did not improve from 2819.05811\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2248.9507 - mae: 31.9162 - val_loss: 3426.6318 - val_mae: 41.1121\n",
      "Epoch 212/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 2583.8779 - mae: 36.3189\n",
      "Epoch 212: val_loss did not improve from 2819.05811\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2583.8779 - mae: 36.3189 - val_loss: 3041.5764 - val_mae: 36.7259\n",
      "Epoch 213/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 2116.7280 - mae: 32.0292\n",
      "Epoch 213: val_loss did not improve from 2819.05811\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2215.6130 - mae: 32.7822 - val_loss: 3002.1975 - val_mae: 35.1730\n",
      "Epoch 214/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 2606.6189 - mae: 35.2425\n",
      "Epoch 214: val_loss did not improve from 2819.05811\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2548.2268 - mae: 35.0154 - val_loss: 3348.5144 - val_mae: 35.7886\n",
      "Epoch 215/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 2569.4751 - mae: 34.4106\n",
      "Epoch 215: val_loss improved from 2819.05811 to 2643.35181, saving model to regressor_weights-215-2643.352.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2555.8599 - mae: 34.4996 - val_loss: 2643.3518 - val_mae: 32.2646\n",
      "Epoch 216/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 2313.9597 - mae: 33.1044\n",
      "Epoch 216: val_loss did not improve from 2643.35181\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2309.4387 - mae: 32.8610 - val_loss: 3878.1887 - val_mae: 35.6599\n",
      "Epoch 217/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 2341.3979 - mae: 33.0638\n",
      "Epoch 217: val_loss did not improve from 2643.35181\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2387.5352 - mae: 33.1237 - val_loss: 3956.2710 - val_mae: 47.1780\n",
      "Epoch 218/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 2551.5349 - mae: 36.7069\n",
      "Epoch 218: val_loss did not improve from 2643.35181\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2574.6284 - mae: 36.8740 - val_loss: 3459.6086 - val_mae: 41.1598\n",
      "Epoch 219/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 2220.6677 - mae: 32.9062\n",
      "Epoch 219: val_loss did not improve from 2643.35181\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2220.6677 - mae: 32.9062 - val_loss: 3110.4146 - val_mae: 40.4632\n",
      "Epoch 220/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1992.8234 - mae: 30.5759\n",
      "Epoch 220: val_loss did not improve from 2643.35181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 6ms/step - loss: 2100.5364 - mae: 31.2501 - val_loss: 3437.6040 - val_mae: 44.3097\n",
      "Epoch 221/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 2100.5725 - mae: 33.2342\n",
      "Epoch 221: val_loss did not improve from 2643.35181\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2150.4473 - mae: 33.3741 - val_loss: 5188.2559 - val_mae: 44.7384\n",
      "Epoch 222/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 2185.2908 - mae: 31.0357\n",
      "Epoch 222: val_loss improved from 2643.35181 to 2523.51562, saving model to regressor_weights-222-2523.516.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 2067.6445 - mae: 30.3329 - val_loss: 2523.5156 - val_mae: 30.2176\n",
      "Epoch 223/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 1767.2172 - mae: 28.2194\n",
      "Epoch 223: val_loss did not improve from 2523.51562\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1780.9038 - mae: 28.1145 - val_loss: 2694.1750 - val_mae: 32.5845\n",
      "Epoch 224/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 2010.1808 - mae: 31.6097\n",
      "Epoch 224: val_loss did not improve from 2523.51562\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1997.3987 - mae: 31.5260 - val_loss: 3268.8093 - val_mae: 36.0295\n",
      "Epoch 225/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 2453.5535 - mae: 35.5392\n",
      "Epoch 225: val_loss did not improve from 2523.51562\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2486.2869 - mae: 35.7901 - val_loss: 5321.9473 - val_mae: 60.0554\n",
      "Epoch 226/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 2234.7073 - mae: 33.5040\n",
      "Epoch 226: val_loss improved from 2523.51562 to 2519.00439, saving model to regressor_weights-226-2519.004.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2222.2363 - mae: 33.4242 - val_loss: 2519.0044 - val_mae: 30.9378\n",
      "Epoch 227/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 2397.8154 - mae: 33.3163\n",
      "Epoch 227: val_loss did not improve from 2519.00439\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2363.6072 - mae: 33.3814 - val_loss: 3127.9724 - val_mae: 34.2370\n",
      "Epoch 228/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1963.9515 - mae: 30.6620\n",
      "Epoch 228: val_loss did not improve from 2519.00439\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1963.5632 - mae: 30.8682 - val_loss: 3490.2454 - val_mae: 41.2958\n",
      "Epoch 229/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 1853.9307 - mae: 29.4629\n",
      "Epoch 229: val_loss did not improve from 2519.00439\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1891.0170 - mae: 29.7163 - val_loss: 3042.6367 - val_mae: 34.2681\n",
      "Epoch 230/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 2574.3237 - mae: 35.0797\n",
      "Epoch 230: val_loss did not improve from 2519.00439\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2574.3237 - mae: 35.0797 - val_loss: 3291.0681 - val_mae: 42.3369\n",
      "Epoch 231/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 2171.4583 - mae: 32.2462\n",
      "Epoch 231: val_loss improved from 2519.00439 to 2096.59448, saving model to regressor_weights-231-2096.594.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 2092.5979 - mae: 31.7073 - val_loss: 2096.5945 - val_mae: 28.7356\n",
      "Epoch 232/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 1682.0513 - mae: 28.3609\n",
      "Epoch 232: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1682.0513 - mae: 28.3609 - val_loss: 2586.7227 - val_mae: 27.1825\n",
      "Epoch 233/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 1863.9452 - mae: 29.3770\n",
      "Epoch 233: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1876.0519 - mae: 29.6278 - val_loss: 2617.2959 - val_mae: 31.7901\n",
      "Epoch 234/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 1776.7322 - mae: 29.0689\n",
      "Epoch 234: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1856.6005 - mae: 29.6023 - val_loss: 3471.5122 - val_mae: 40.6674\n",
      "Epoch 235/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 1857.1122 - mae: 30.7754\n",
      "Epoch 235: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1874.4810 - mae: 30.7800 - val_loss: 2808.7886 - val_mae: 34.0271\n",
      "Epoch 236/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 1854.0343 - mae: 30.7301\n",
      "Epoch 236: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1871.2681 - mae: 30.6576 - val_loss: 2433.1865 - val_mae: 28.4003\n",
      "Epoch 237/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 2411.9729 - mae: 34.6380\n",
      "Epoch 237: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2433.2864 - mae: 35.4028 - val_loss: 3873.7600 - val_mae: 41.0313\n",
      "Epoch 238/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 2891.8589 - mae: 39.0201\n",
      "Epoch 238: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2783.4688 - mae: 37.9113 - val_loss: 2175.0325 - val_mae: 28.4992\n",
      "Epoch 239/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 2082.2078 - mae: 32.7729\n",
      "Epoch 239: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2065.3901 - mae: 32.6415 - val_loss: 3473.6099 - val_mae: 37.0831\n",
      "Epoch 240/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 1734.4657 - mae: 29.1005\n",
      "Epoch 240: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1728.3073 - mae: 29.4576 - val_loss: 3367.1953 - val_mae: 39.4846\n",
      "Epoch 241/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 2583.9443 - mae: 35.0272\n",
      "Epoch 241: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2543.6829 - mae: 34.9084 - val_loss: 4830.7471 - val_mae: 54.0176\n",
      "Epoch 242/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 1699.0282 - mae: 28.4714\n",
      "Epoch 242: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1739.4482 - mae: 28.9538 - val_loss: 2274.6584 - val_mae: 27.3587\n",
      "Epoch 243/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1751.9664 - mae: 29.8271\n",
      "Epoch 243: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1812.2229 - mae: 29.8771 - val_loss: 2304.3813 - val_mae: 27.3491\n",
      "Epoch 244/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 1594.7472 - mae: 28.1877\n",
      "Epoch 244: val_loss did not improve from 2096.59448\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1611.5071 - mae: 28.4416 - val_loss: 3303.2375 - val_mae: 42.7044\n",
      "Epoch 245/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 2245.6040 - mae: 32.7337\n",
      "Epoch 245: val_loss improved from 2096.59448 to 2057.43872, saving model to regressor_weights-245-2057.439.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2245.6040 - mae: 32.7337 - val_loss: 2057.4387 - val_mae: 25.0275\n",
      "Epoch 246/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1960.6284 - mae: 31.4692\n",
      "Epoch 246: val_loss did not improve from 2057.43872\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1934.6530 - mae: 31.2518 - val_loss: 2434.1387 - val_mae: 32.8980\n",
      "Epoch 247/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1612.4344 - mae: 28.0727\n",
      "Epoch 247: val_loss did not improve from 2057.43872\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1580.0387 - mae: 27.7515 - val_loss: 2580.7859 - val_mae: 36.9123\n",
      "Epoch 248/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 1458.6274 - mae: 26.2528\n",
      "Epoch 248: val_loss improved from 2057.43872 to 1982.47913, saving model to regressor_weights-248-1982.479.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1560.9906 - mae: 27.0322 - val_loss: 1982.4791 - val_mae: 26.5948\n",
      "Epoch 249/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 1585.2716 - mae: 27.6533\n",
      "Epoch 249: val_loss did not improve from 1982.47913\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1569.9838 - mae: 27.5755 - val_loss: 2073.0278 - val_mae: 24.9457\n",
      "Epoch 250/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 1464.5005 - mae: 25.9687\n",
      "Epoch 250: val_loss did not improve from 1982.47913\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1477.7297 - mae: 26.1064 - val_loss: 2012.5358 - val_mae: 27.3105\n",
      "Epoch 251/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 1399.8873 - mae: 26.0497\n",
      "Epoch 251: val_loss did not improve from 1982.47913\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1336.2537 - mae: 25.4896 - val_loss: 2033.6134 - val_mae: 25.4881\n",
      "Epoch 252/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 1525.9758 - mae: 26.9566\n",
      "Epoch 252: val_loss did not improve from 1982.47913\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1515.1317 - mae: 26.7956 - val_loss: 2460.5398 - val_mae: 27.0928\n",
      "Epoch 253/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 1596.1454 - mae: 28.4027\n",
      "Epoch 253: val_loss did not improve from 1982.47913\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1596.0118 - mae: 28.2748 - val_loss: 2409.9763 - val_mae: 29.1957\n",
      "Epoch 254/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 2126.0432 - mae: 32.3289\n",
      "Epoch 254: val_loss did not improve from 1982.47913\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 2074.2600 - mae: 31.8656 - val_loss: 2935.6621 - val_mae: 31.6654\n",
      "Epoch 255/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 2243.5552 - mae: 34.5554\n",
      "Epoch 255: val_loss improved from 1982.47913 to 1696.10352, saving model to regressor_weights-255-1696.104.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2192.7825 - mae: 34.3377 - val_loss: 1696.1035 - val_mae: 23.7757\n",
      "Epoch 256/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 1688.5212 - mae: 29.8000\n",
      "Epoch 256: val_loss did not improve from 1696.10352\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1610.0558 - mae: 28.9185 - val_loss: 2647.0178 - val_mae: 28.4290\n",
      "Epoch 257/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 1722.6953 - mae: 29.7279\n",
      "Epoch 257: val_loss did not improve from 1696.10352\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1715.1554 - mae: 29.6266 - val_loss: 1819.6597 - val_mae: 25.1108\n",
      "Epoch 258/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 2289.0872 - mae: 34.0378\n",
      "Epoch 258: val_loss did not improve from 1696.10352\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 2249.4734 - mae: 33.8637 - val_loss: 2359.5876 - val_mae: 31.0336\n",
      "Epoch 259/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 1489.4875 - mae: 26.1725\n",
      "Epoch 259: val_loss did not improve from 1696.10352\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1489.4875 - mae: 26.1725 - val_loss: 2400.2261 - val_mae: 32.0283\n",
      "Epoch 260/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 1766.1812 - mae: 28.6202\n",
      "Epoch 260: val_loss did not improve from 1696.10352\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1705.3890 - mae: 27.9779 - val_loss: 2051.7336 - val_mae: 28.1442\n",
      "Epoch 261/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 1230.1500 - mae: 24.4636\n",
      "Epoch 261: val_loss did not improve from 1696.10352\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1230.1500 - mae: 24.4636 - val_loss: 1809.3799 - val_mae: 24.9268\n",
      "Epoch 262/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 1285.8417 - mae: 24.6529\n",
      "Epoch 262: val_loss did not improve from 1696.10352\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1283.7965 - mae: 24.8466 - val_loss: 2209.1636 - val_mae: 28.1076\n",
      "Epoch 263/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 1240.1487 - mae: 24.0096\n",
      "Epoch 263: val_loss did not improve from 1696.10352\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1240.1487 - mae: 24.0096 - val_loss: 3138.9470 - val_mae: 43.3370\n",
      "Epoch 264/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1948.5476 - mae: 31.9196\n",
      "Epoch 264: val_loss improved from 1696.10352 to 1643.50476, saving model to regressor_weights-264-1643.505.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1911.0604 - mae: 31.5793 - val_loss: 1643.5048 - val_mae: 24.4946\n",
      "Epoch 265/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 1378.2761 - mae: 25.8338\n",
      "Epoch 265: val_loss did not improve from 1643.50476\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1372.1057 - mae: 25.8006 - val_loss: 2183.2009 - val_mae: 31.2972\n",
      "Epoch 266/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 1153.5657 - mae: 23.6323\n",
      "Epoch 266: val_loss did not improve from 1643.50476\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1153.0161 - mae: 23.8959 - val_loss: 1810.5625 - val_mae: 23.3000\n",
      "Epoch 267/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 1363.1240 - mae: 25.9700\n",
      "Epoch 267: val_loss did not improve from 1643.50476\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1359.4089 - mae: 25.9251 - val_loss: 2303.0388 - val_mae: 31.0063\n",
      "Epoch 268/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 1275.3746 - mae: 24.5571\n",
      "Epoch 268: val_loss did not improve from 1643.50476\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1275.3746 - mae: 24.5571 - val_loss: 3268.6133 - val_mae: 35.8012\n",
      "Epoch 269/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1590.0648 - mae: 27.2462\n",
      "Epoch 269: val_loss did not improve from 1643.50476\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1573.1858 - mae: 26.8859 - val_loss: 2413.2800 - val_mae: 36.3211\n",
      "Epoch 270/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 1439.3409 - mae: 26.6894\n",
      "Epoch 270: val_loss did not improve from 1643.50476\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1469.6647 - mae: 26.9291 - val_loss: 2245.0791 - val_mae: 27.8187\n",
      "Epoch 271/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 1689.7678 - mae: 29.0269\n",
      "Epoch 271: val_loss improved from 1643.50476 to 1590.92688, saving model to regressor_weights-271-1590.927.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1689.7678 - mae: 29.0269 - val_loss: 1590.9269 - val_mae: 23.0601\n",
      "Epoch 272/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 1513.6311 - mae: 27.5214\n",
      "Epoch 272: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1513.6311 - mae: 27.5214 - val_loss: 2111.2578 - val_mae: 32.7381\n",
      "Epoch 273/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 1102.7067 - mae: 22.8310\n",
      "Epoch 273: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1125.4104 - mae: 23.0491 - val_loss: 1762.0186 - val_mae: 24.9408\n",
      "Epoch 274/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 2321.6709 - mae: 32.1718\n",
      "Epoch 274: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 2318.6409 - mae: 32.1412 - val_loss: 1701.2856 - val_mae: 23.0397\n",
      "Epoch 275/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 1627.8263 - mae: 26.3754\n",
      "Epoch 275: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1656.2963 - mae: 26.9919 - val_loss: 2134.2285 - val_mae: 30.2530\n",
      "Epoch 276/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 1637.1165 - mae: 28.1138\n",
      "Epoch 276: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1694.5135 - mae: 28.4914 - val_loss: 2666.6279 - val_mae: 39.0767\n",
      "Epoch 277/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 1730.8390 - mae: 28.7000\n",
      "Epoch 277: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1664.3024 - mae: 28.1902 - val_loss: 1612.4176 - val_mae: 23.6817\n",
      "Epoch 278/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/80 [==========================>...] - ETA: 0s - loss: 1082.0426 - mae: 22.6049\n",
      "Epoch 278: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1111.5897 - mae: 22.8134 - val_loss: 2612.3879 - val_mae: 37.7537\n",
      "Epoch 279/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 1282.8102 - mae: 24.3111\n",
      "Epoch 279: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1230.5934 - mae: 23.9354 - val_loss: 2498.9521 - val_mae: 35.0809\n",
      "Epoch 280/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 1324.4905 - mae: 26.3626\n",
      "Epoch 280: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1349.2717 - mae: 26.5737 - val_loss: 1919.6487 - val_mae: 31.5343\n",
      "Epoch 281/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 1434.7545 - mae: 27.4767\n",
      "Epoch 281: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1511.2975 - mae: 27.8311 - val_loss: 1953.9058 - val_mae: 24.2535\n",
      "Epoch 282/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 1513.0328 - mae: 26.9294\n",
      "Epoch 282: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1513.0328 - mae: 26.9294 - val_loss: 2385.4363 - val_mae: 29.4430\n",
      "Epoch 283/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 1551.1276 - mae: 27.4889\n",
      "Epoch 283: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1551.1276 - mae: 27.4889 - val_loss: 1946.3052 - val_mae: 25.4209\n",
      "Epoch 284/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1592.2449 - mae: 27.4815\n",
      "Epoch 284: val_loss did not improve from 1590.92688\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1567.0596 - mae: 27.3411 - val_loss: 2829.2354 - val_mae: 38.3190\n",
      "Epoch 285/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 1097.1284 - mae: 22.7859\n",
      "Epoch 285: val_loss improved from 1590.92688 to 1553.80261, saving model to regressor_weights-285-1553.803.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1097.5435 - mae: 22.8212 - val_loss: 1553.8026 - val_mae: 26.2393\n",
      "Epoch 286/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1133.4971 - mae: 23.9138\n",
      "Epoch 286: val_loss did not improve from 1553.80261\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1157.1300 - mae: 24.2121 - val_loss: 3051.3696 - val_mae: 39.8702\n",
      "Epoch 287/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1468.3125 - mae: 26.3920\n",
      "Epoch 287: val_loss did not improve from 1553.80261\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1420.5562 - mae: 26.0013 - val_loss: 1784.2847 - val_mae: 25.1036\n",
      "Epoch 288/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 1241.7152 - mae: 24.2789\n",
      "Epoch 288: val_loss did not improve from 1553.80261\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1241.8688 - mae: 24.2052 - val_loss: 2087.4192 - val_mae: 28.7076\n",
      "Epoch 289/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 973.1803 - mae: 21.4722\n",
      "Epoch 289: val_loss did not improve from 1553.80261\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 969.0043 - mae: 21.4733 - val_loss: 1564.7828 - val_mae: 23.4798\n",
      "Epoch 290/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 965.2160 - mae: 21.7382\n",
      "Epoch 290: val_loss did not improve from 1553.80261\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 964.5098 - mae: 21.6946 - val_loss: 1925.3960 - val_mae: 29.5945\n",
      "Epoch 291/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 1037.8923 - mae: 22.5350\n",
      "Epoch 291: val_loss improved from 1553.80261 to 1427.13879, saving model to regressor_weights-291-1427.139.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1015.0714 - mae: 22.2908 - val_loss: 1427.1388 - val_mae: 20.9274\n",
      "Epoch 292/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 1012.6205 - mae: 22.1630\n",
      "Epoch 292: val_loss did not improve from 1427.13879\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1004.0709 - mae: 22.0730 - val_loss: 1593.7515 - val_mae: 21.3512\n",
      "Epoch 293/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 1086.8768 - mae: 21.3361\n",
      "Epoch 293: val_loss improved from 1427.13879 to 1399.32141, saving model to regressor_weights-293-1399.321.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1088.0172 - mae: 21.3034 - val_loss: 1399.3214 - val_mae: 22.2057\n",
      "Epoch 294/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 950.0401 - mae: 21.0296\n",
      "Epoch 294: val_loss did not improve from 1399.32141\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 938.1571 - mae: 20.8405 - val_loss: 1970.9573 - val_mae: 25.6401\n",
      "Epoch 295/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 821.2801 - mae: 19.5387\n",
      "Epoch 295: val_loss improved from 1399.32141 to 1376.69897, saving model to regressor_weights-295-1376.699.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 818.9378 - mae: 19.5182 - val_loss: 1376.6990 - val_mae: 19.9245\n",
      "Epoch 296/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 1165.7692 - mae: 24.1224\n",
      "Epoch 296: val_loss did not improve from 1376.69897\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1340.5894 - mae: 25.9072 - val_loss: 3394.3674 - val_mae: 42.7881\n",
      "Epoch 297/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 1419.8026 - mae: 27.3626\n",
      "Epoch 297: val_loss did not improve from 1376.69897\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1425.5563 - mae: 27.3336 - val_loss: 1832.4775 - val_mae: 29.3682\n",
      "Epoch 298/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 1474.7979 - mae: 28.6502\n",
      "Epoch 298: val_loss improved from 1376.69897 to 1327.80908, saving model to regressor_weights-298-1327.809.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1387.1393 - mae: 27.6419 - val_loss: 1327.8091 - val_mae: 20.8985\n",
      "Epoch 299/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 761.3486 - mae: 18.9428\n",
      "Epoch 299: val_loss did not improve from 1327.80908\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 767.7782 - mae: 18.9905 - val_loss: 1650.2323 - val_mae: 22.7431\n",
      "Epoch 300/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 1026.5590 - mae: 22.4598\n",
      "Epoch 300: val_loss improved from 1327.80908 to 1288.02039, saving model to regressor_weights-300-1288.020.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1002.4257 - mae: 22.2956 - val_loss: 1288.0204 - val_mae: 20.5095\n",
      "Epoch 301/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 908.3579 - mae: 20.9169\n",
      "Epoch 301: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 907.1094 - mae: 20.9002 - val_loss: 1597.2150 - val_mae: 21.9652\n",
      "Epoch 302/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 1105.6792 - mae: 23.3598\n",
      "Epoch 302: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1132.6331 - mae: 23.6936 - val_loss: 1945.5203 - val_mae: 31.6535\n",
      "Epoch 303/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 1187.8558 - mae: 25.9159\n",
      "Epoch 303: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1217.5222 - mae: 26.3190 - val_loss: 2238.6890 - val_mae: 33.5828\n",
      "Epoch 304/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 1228.5861 - mae: 25.6089\n",
      "Epoch 304: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1250.1926 - mae: 25.8543 - val_loss: 2138.0803 - val_mae: 29.2141\n",
      "Epoch 305/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 908.3646 - mae: 21.0569\n",
      "Epoch 305: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 958.8540 - mae: 21.7586 - val_loss: 1522.5768 - val_mae: 21.5435\n",
      "Epoch 306/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 825.7337 - mae: 19.8346\n",
      "Epoch 306: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 854.7133 - mae: 20.0940 - val_loss: 2459.5271 - val_mae: 35.6487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 884.2327 - mae: 20.3246\n",
      "Epoch 307: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 896.7888 - mae: 20.5028 - val_loss: 1410.4567 - val_mae: 20.5427\n",
      "Epoch 308/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 796.3153 - mae: 19.2884\n",
      "Epoch 308: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 796.3153 - mae: 19.2884 - val_loss: 1473.5800 - val_mae: 20.3723\n",
      "Epoch 309/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 739.5977 - mae: 18.5197\n",
      "Epoch 309: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 747.1998 - mae: 18.5864 - val_loss: 1692.6864 - val_mae: 26.7021\n",
      "Epoch 310/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 1125.7828 - mae: 23.8248\n",
      "Epoch 310: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1119.5775 - mae: 23.6657 - val_loss: 1387.7737 - val_mae: 21.9571\n",
      "Epoch 311/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 1168.8368 - mae: 24.1599\n",
      "Epoch 311: val_loss did not improve from 1288.02039\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1118.0266 - mae: 23.7091 - val_loss: 1932.0034 - val_mae: 30.6071\n",
      "Epoch 312/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1124.1281 - mae: 24.2441\n",
      "Epoch 312: val_loss improved from 1288.02039 to 1233.58838, saving model to regressor_weights-312-1233.588.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1113.6960 - mae: 23.9500 - val_loss: 1233.5884 - val_mae: 19.3448\n",
      "Epoch 313/350\n",
      "70/80 [=========================>....] - ETA: 0s - loss: 763.4435 - mae: 18.7972\n",
      "Epoch 313: val_loss did not improve from 1233.58838\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 751.6117 - mae: 18.7328 - val_loss: 1284.9498 - val_mae: 19.6495\n",
      "Epoch 314/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 784.9381 - mae: 19.4791\n",
      "Epoch 314: val_loss did not improve from 1233.58838\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 780.6959 - mae: 19.5299 - val_loss: 1653.0955 - val_mae: 25.4327\n",
      "Epoch 315/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 1059.6907 - mae: 23.5193\n",
      "Epoch 315: val_loss did not improve from 1233.58838\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1010.6072 - mae: 22.8258 - val_loss: 1378.4463 - val_mae: 19.5149\n",
      "Epoch 316/350\n",
      "76/80 [===========================>..] - ETA: 0s - loss: 729.6227 - mae: 18.6652\n",
      "Epoch 316: val_loss did not improve from 1233.58838\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 757.6572 - mae: 19.1131 - val_loss: 2152.1265 - val_mae: 28.2337\n",
      "Epoch 317/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 1629.6476 - mae: 30.5410\n",
      "Epoch 317: val_loss did not improve from 1233.58838\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1632.4497 - mae: 30.4214 - val_loss: 1912.8048 - val_mae: 23.2829\n",
      "Epoch 318/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 3330.8110 - mae: 41.3611\n",
      "Epoch 318: val_loss improved from 1233.58838 to 1227.09900, saving model to regressor_weights-318-1227.099.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3330.8110 - mae: 41.3611 - val_loss: 1227.0990 - val_mae: 19.0070\n",
      "Epoch 319/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1020.5293 - mae: 21.9831\n",
      "Epoch 319: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1052.9945 - mae: 22.4954 - val_loss: 1429.8992 - val_mae: 22.5532\n",
      "Epoch 320/350\n",
      "77/80 [===========================>..] - ETA: 0s - loss: 990.2108 - mae: 22.1682\n",
      "Epoch 320: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 995.2682 - mae: 22.2358 - val_loss: 1368.6864 - val_mae: 21.7286\n",
      "Epoch 321/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 740.6024 - mae: 18.2209\n",
      "Epoch 321: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 735.0356 - mae: 18.1258 - val_loss: 1383.7961 - val_mae: 19.5260\n",
      "Epoch 322/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 752.5143 - mae: 18.5352\n",
      "Epoch 322: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 763.6416 - mae: 18.5904 - val_loss: 1627.7123 - val_mae: 23.6866\n",
      "Epoch 323/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 656.6492 - mae: 16.9209\n",
      "Epoch 323: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 661.5346 - mae: 17.0653 - val_loss: 1264.4202 - val_mae: 17.5715\n",
      "Epoch 324/350\n",
      "79/80 [============================>.] - ETA: 0s - loss: 730.3694 - mae: 18.1895\n",
      "Epoch 324: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 727.4406 - mae: 18.1527 - val_loss: 1305.3613 - val_mae: 18.4607\n",
      "Epoch 325/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 651.5366 - mae: 17.1438\n",
      "Epoch 325: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 659.9488 - mae: 17.3183 - val_loss: 1449.8442 - val_mae: 21.4595\n",
      "Epoch 326/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 689.4116 - mae: 18.1455\n",
      "Epoch 326: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 723.3284 - mae: 18.6083 - val_loss: 1529.8300 - val_mae: 24.3213\n",
      "Epoch 327/350\n",
      "75/80 [===========================>..] - ETA: 0s - loss: 810.6590 - mae: 20.0662\n",
      "Epoch 327: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 808.9097 - mae: 20.1169 - val_loss: 1707.6467 - val_mae: 24.4538\n",
      "Epoch 328/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 689.1965 - mae: 18.4656\n",
      "Epoch 328: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 695.5989 - mae: 18.4992 - val_loss: 1257.5422 - val_mae: 21.1339\n",
      "Epoch 329/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 816.5288 - mae: 20.1657\n",
      "Epoch 329: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 807.1086 - mae: 20.1773 - val_loss: 1353.7592 - val_mae: 24.3721\n",
      "Epoch 330/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1355.1426 - mae: 25.2052\n",
      "Epoch 330: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1344.0233 - mae: 25.2674 - val_loss: 1300.7876 - val_mae: 22.9265\n",
      "Epoch 331/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 1071.2916 - mae: 23.0706\n",
      "Epoch 331: val_loss did not improve from 1227.09900\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1061.3141 - mae: 22.8764 - val_loss: 1762.1349 - val_mae: 20.7695\n",
      "Epoch 332/350\n",
      "74/80 [==========================>...] - ETA: 0s - loss: 789.0519 - mae: 19.1010\n",
      "Epoch 332: val_loss improved from 1227.09900 to 1180.59814, saving model to regressor_weights-332-1180.598.hdf5\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 764.6959 - mae: 18.8217 - val_loss: 1180.5981 - val_mae: 19.1362\n",
      "Epoch 333/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 716.3111 - mae: 18.7680\n",
      "Epoch 333: val_loss did not improve from 1180.59814\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 725.3209 - mae: 19.1270 - val_loss: 1284.2356 - val_mae: 20.9421\n",
      "Epoch 334/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 725.6735 - mae: 19.2124\n",
      "Epoch 334: val_loss did not improve from 1180.59814\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 747.0513 - mae: 19.4179 - val_loss: 1561.0117 - val_mae: 20.3717\n",
      "Epoch 335/350\n",
      "69/80 [========================>.....] - ETA: 0s - loss: 796.4521 - mae: 19.5539\n",
      "Epoch 335: val_loss improved from 1180.59814 to 1168.22241, saving model to regressor_weights-335-1168.222.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 790.8794 - mae: 19.5851 - val_loss: 1168.2224 - val_mae: 16.3420\n",
      "Epoch 336/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 953.3543 - mae: 21.9423\n",
      "Epoch 336: val_loss did not improve from 1168.22241\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 954.6682 - mae: 21.9570 - val_loss: 1295.0427 - val_mae: 23.1587\n",
      "Epoch 337/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 884.1991 - mae: 21.4638\n",
      "Epoch 337: val_loss did not improve from 1168.22241\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 873.1633 - mae: 21.3215 - val_loss: 1225.1069 - val_mae: 23.0618\n",
      "Epoch 338/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 756.2512 - mae: 19.3394\n",
      "Epoch 338: val_loss did not improve from 1168.22241\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 768.9896 - mae: 19.4724 - val_loss: 1460.0798 - val_mae: 20.6670\n",
      "Epoch 339/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 1096.5891 - mae: 25.0116\n",
      "Epoch 339: val_loss did not improve from 1168.22241\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1090.8258 - mae: 24.9529 - val_loss: 1267.9713 - val_mae: 22.4405\n",
      "Epoch 340/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 687.8968 - mae: 17.9093\n",
      "Epoch 340: val_loss did not improve from 1168.22241\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 702.4241 - mae: 18.2748 - val_loss: 1668.0361 - val_mae: 28.1297\n",
      "Epoch 341/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 818.4285 - mae: 20.4968\n",
      "Epoch 341: val_loss did not improve from 1168.22241\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 820.1248 - mae: 20.6380 - val_loss: 1474.1300 - val_mae: 25.4259\n",
      "Epoch 342/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 1333.1740 - mae: 27.2549\n",
      "Epoch 342: val_loss did not improve from 1168.22241\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1248.9860 - mae: 26.1446 - val_loss: 1637.4370 - val_mae: 20.2945\n",
      "Epoch 343/350\n",
      "71/80 [=========================>....] - ETA: 0s - loss: 654.4122 - mae: 16.9902\n",
      "Epoch 343: val_loss did not improve from 1168.22241\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 684.9915 - mae: 17.5970 - val_loss: 1413.1656 - val_mae: 25.2506\n",
      "Epoch 344/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 893.5897 - mae: 21.1263\n",
      "Epoch 344: val_loss improved from 1168.22241 to 1109.91821, saving model to regressor_weights-344-1109.918.hdf5\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 863.8207 - mae: 20.7312 - val_loss: 1109.9182 - val_mae: 18.9070\n",
      "Epoch 345/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 629.9026 - mae: 17.4390\n",
      "Epoch 345: val_loss did not improve from 1109.91821\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 665.3446 - mae: 18.0488 - val_loss: 1355.1189 - val_mae: 21.1794\n",
      "Epoch 346/350\n",
      "73/80 [==========================>...] - ETA: 0s - loss: 640.4892 - mae: 17.1115\n",
      "Epoch 346: val_loss did not improve from 1109.91821\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 655.5838 - mae: 17.2886 - val_loss: 1293.9854 - val_mae: 18.7964\n",
      "Epoch 347/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 749.5672 - mae: 18.8143\n",
      "Epoch 347: val_loss did not improve from 1109.91821\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 758.2245 - mae: 18.9480 - val_loss: 1178.9469 - val_mae: 17.4579\n",
      "Epoch 348/350\n",
      "80/80 [==============================] - ETA: 0s - loss: 574.8632 - mae: 15.5751\n",
      "Epoch 348: val_loss improved from 1109.91821 to 1095.96777, saving model to regressor_weights-348-1095.968.hdf5\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 574.8632 - mae: 15.5751 - val_loss: 1095.9678 - val_mae: 16.2032\n",
      "Epoch 349/350\n",
      "78/80 [============================>.] - ETA: 0s - loss: 910.0883 - mae: 22.3236\n",
      "Epoch 349: val_loss did not improve from 1095.96777\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 904.2895 - mae: 22.2381 - val_loss: 1121.0232 - val_mae: 18.2806\n",
      "Epoch 350/350\n",
      "72/80 [==========================>...] - ETA: 0s - loss: 1041.4366 - mae: 22.3303\n",
      "Epoch 350: val_loss did not improve from 1095.96777\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1040.6791 - mae: 22.3723 - val_loss: 1719.6119 - val_mae: 26.8785\n"
     ]
    }
   ],
   "source": [
    "regressor_history = regressor.fit(X_train, y_train, batch_size=40, validation_data=(X_val, y_val),\n",
    "callbacks=callbacks_list, epochs=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbdb66",
   "metadata": {},
   "source": [
    "#### Построение графика потери\n",
    "Если функция потерь на тренировочной выборке продолжает уменьшаться, в то время как на валидационной выборке начинает возрастать, это является признаком переобучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4e0c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_function = regressor_history.history['loss']\n",
    "val_loss_function = regressor_history.history['val_loss']\n",
    "epochs = range(1, len(loss_function)+1)\n",
    "\n",
    "plt.title('Loss function (Train & Val Sets)')\n",
    "plt.plot(epochs, loss_function, label='Train Loss')\n",
    "plt.plot(epochs, val_loss_function, color='orange', label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c84941c",
   "metadata": {},
   "source": [
    "#### График изменения величины средней абсолютной ошибки (Mean Absolute Error, MAE) модели в процессе\n",
    "Две кривые: одна для обучающего набора данных (\"Mean Absolute Error (Train)\") и для валидационного набора данных (\"Mean Absolute Error (Validation)\").\n",
    "Если на графике видно, что ошибка на обучающей выборке продолжает уменьшаться, в то время как ошибка на валидационной выборке начинает увеличиваться, это может свидетельствовать о переобучении модели, когда она хорошо обучается на тренировочных данных, но плохо справляется с новыми, наблюдаемыми во время валидации данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e8b1a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae = regressor_history.history['mae']\n",
    "val_mae = regressor_history.history['val_mae']\n",
    "epochs = range(1, len(mae)+1)\n",
    "\n",
    "plt.title('MAE (Train & Val Sets)')\n",
    "plt.plot(epochs, mae, label='Mean Absolute Error (Train)')\n",
    "plt.plot(epochs, val_mae, color='orange', label='Mean Absolute Error (Validation)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fae0546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step - loss: 1588.5815 - mae: 28.0636\n"
     ]
    }
   ],
   "source": [
    "results = regressor.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c633c0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6901      3540\n",
       "4670      7372\n",
       "6134     10914\n",
       "2480     10781\n",
       "8744      4837\n",
       "         ...  \n",
       "5834       688\n",
       "10531    15046\n",
       "1912     12806\n",
       "6279      2744\n",
       "3881     10903\n",
       "Name: TOTALBTUCOL, Length: 995, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36363472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[10929.391]\n"
     ]
    }
   ],
   "source": [
    "x_test_pattern = X_test[2, :]\n",
    "y_pred = regressor.predict(x_test_pattern.reshape(1, -1))\n",
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21e4cab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00000000e+00, 7.00000000e+00, 2.10000000e+01, 4.61011000e+02,\n",
       "        4.61000000e+02, 3.19867000e+03, 1.09138550e+04, 3.02000000e+02,\n",
       "        7.16000000e+03, 0.00000000e+00, 8.10162822e+03, 1.53600000e+03,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.25600000e+03, 9.28817000e+02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.00000000e+01,\n",
       "        1.53600000e+03, 0.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 9.29000000e+02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.09852000e+03,\n",
       "        7.16014700e+03, 0.00000000e+00, 0.00000000e+00, 4.00000000e+02,\n",
       "        2.25600000e+03, 1.00000000e+00, 3.02451000e+02, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.00000000e+02, 1.93600000e+03, 1.53600000e+03, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 5.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.00000000e+00,\n",
       "        0.00000000e+00, 5.34080000e+04, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.62294000e+02, 2.51373900e+03,\n",
       "        8.57687100e+03, 6.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.19890000e+04, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.00000000e+00, 0.00000000e+00, 3.00000000e+00, 1.00000000e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.44448900e+03,\n",
       "        0.00000000e+00, 3.62000000e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.98200000e+03, 2.19885820e+04, 1.56530000e+04, 5.34080000e+04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.50000000e+01, 1.00000000e+00, 8.57700000e+03,\n",
       "        5.00000000e+00, 2.01427000e+02, 2.01000000e+02, 6.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.00000000e+00, 1.39758200e+03]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_features = min_max_scaler.inverse_transform(x_test_pattern.reshape(1, -1))\n",
    "original_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aec8708c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.66666667, 0.76923077, 0.05960983, 0.05964549,\n",
       "       0.0523986 , 0.05239854, 0.03259931, 0.017862  , 0.        ,\n",
       "       0.17477496, 0.10244189, 0.        , 0.1       , 0.11532785,\n",
       "       0.09676711, 0.        , 0.        , 0.        , 0.66666667,\n",
       "       0.1055247 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.06945639, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.15159057, 0.1515907 , 0.        , 0.        ,\n",
       "       0.05270787, 0.0682907 , 0.        , 0.22387924, 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.42857143,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0514668 , 0.09331459, 0.07118102,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.2       ,\n",
       "       0.        , 0.        , 0.        , 0.13333333, 0.        ,\n",
       "       0.04603602, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.20628734, 0.2004597 , 0.20045939, 0.19047619, 1.        ,\n",
       "       0.        , 0.        , 0.05070514, 0.        , 0.        ,\n",
       "       0.25      , 0.        , 0.05263158, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2       , 0.        ,\n",
       "       0.        , 0.14285714, 0.        , 0.        , 0.        ,\n",
       "       0.09784827, 0.        , 0.11990725, 0.        , 0.        ,\n",
       "       0.69662921, 0.0978495 , 0.10359226, 0.10359278, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.55555556,\n",
       "       0.025     , 0.03018689, 0.57142857, 0.08089288, 0.08072289,\n",
       "       0.22727273, 0.        , 0.        , 1.        , 0.1262606 ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5969a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8c3e3",
   "metadata": {},
   "source": [
    "### Метрики работы MSE, MAE, R2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff4b9085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1588.583464797732\n",
      "MAE: 28.06361318617011\n",
      "R2: 0.9999828651563029\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ca0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
